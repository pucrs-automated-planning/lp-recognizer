diff -X lp-recognizer/fd-patch.ignore -ruN fast-downward-original/experiments/issue851/common_setup.py fast-downward/experiments/issue851/common_setup.py
--- fast-downward-original/experiments/issue851/common_setup.py	1969-12-31 21:00:00.000000000 -0300
+++ fast-downward/experiments/issue851/common_setup.py	2019-02-11 14:13:33.000000000 -0200
@@ -0,0 +1,395 @@
+# -*- coding: utf-8 -*-
+
+import itertools
+import os
+import platform
+import subprocess
+import sys
+
+from lab.experiment import ARGPARSER
+from lab import tools
+
+from downward.experiment import FastDownwardExperiment
+from downward.reports.absolute import AbsoluteReport
+from downward.reports.compare import ComparativeReport
+from downward.reports.scatter import ScatterPlotReport
+
+from relativescatter import RelativeScatterPlotReport
+
+
+def parse_args():
+    ARGPARSER.add_argument(
+        "--test",
+        choices=["yes", "no", "auto"],
+        default="auto",
+        dest="test_run",
+        help="test experiment locally on a small suite if --test=yes or "
+             "--test=auto and we are not on a cluster")
+    return ARGPARSER.parse_args()
+
+ARGS = parse_args()
+
+
+DEFAULT_OPTIMAL_SUITE = [
+    'agricola-opt18-strips', 'airport', 'barman-opt11-strips',
+    'barman-opt14-strips', 'blocks', 'childsnack-opt14-strips',
+    'data-network-opt18-strips', 'depot', 'driverlog',
+    'elevators-opt08-strips', 'elevators-opt11-strips',
+    'floortile-opt11-strips', 'floortile-opt14-strips', 'freecell',
+    'ged-opt14-strips', 'grid', 'gripper', 'hiking-opt14-strips',
+    'logistics00', 'logistics98', 'miconic', 'movie', 'mprime',
+    'mystery', 'nomystery-opt11-strips', 'openstacks-opt08-strips',
+    'openstacks-opt11-strips', 'openstacks-opt14-strips',
+    'openstacks-strips', 'organic-synthesis-opt18-strips',
+    'organic-synthesis-split-opt18-strips', 'parcprinter-08-strips',
+    'parcprinter-opt11-strips', 'parking-opt11-strips',
+    'parking-opt14-strips', 'pathways-noneg', 'pegsol-08-strips',
+    'pegsol-opt11-strips', 'petri-net-alignment-opt18-strips',
+    'pipesworld-notankage', 'pipesworld-tankage', 'psr-small', 'rovers',
+    'satellite', 'scanalyzer-08-strips', 'scanalyzer-opt11-strips',
+    'snake-opt18-strips', 'sokoban-opt08-strips',
+    'sokoban-opt11-strips', 'spider-opt18-strips', 'storage',
+    'termes-opt18-strips', 'tetris-opt14-strips',
+    'tidybot-opt11-strips', 'tidybot-opt14-strips', 'tpp',
+    'transport-opt08-strips', 'transport-opt11-strips',
+    'transport-opt14-strips', 'trucks-strips', 'visitall-opt11-strips',
+    'visitall-opt14-strips', 'woodworking-opt08-strips',
+    'woodworking-opt11-strips', 'zenotravel']
+
+DEFAULT_SATISFICING_SUITE = [
+    'agricola-sat18-strips', 'airport', 'assembly',
+    'barman-sat11-strips', 'barman-sat14-strips', 'blocks',
+    'caldera-sat18-adl', 'caldera-split-sat18-adl', 'cavediving-14-adl',
+    'childsnack-sat14-strips', 'citycar-sat14-adl',
+    'data-network-sat18-strips', 'depot', 'driverlog',
+    'elevators-sat08-strips', 'elevators-sat11-strips',
+    'flashfill-sat18-adl', 'floortile-sat11-strips',
+    'floortile-sat14-strips', 'freecell', 'ged-sat14-strips', 'grid',
+    'gripper', 'hiking-sat14-strips', 'logistics00', 'logistics98',
+    'maintenance-sat14-adl', 'miconic', 'miconic-fulladl',
+    'miconic-simpleadl', 'movie', 'mprime', 'mystery',
+    'nomystery-sat11-strips', 'nurikabe-sat18-adl', 'openstacks',
+    'openstacks-sat08-adl', 'openstacks-sat08-strips',
+    'openstacks-sat11-strips', 'openstacks-sat14-strips',
+    'openstacks-strips', 'optical-telegraphs',
+    'organic-synthesis-sat18-strips',
+    'organic-synthesis-split-sat18-strips', 'parcprinter-08-strips',
+    'parcprinter-sat11-strips', 'parking-sat11-strips',
+    'parking-sat14-strips', 'pathways', 'pathways-noneg',
+    'pegsol-08-strips', 'pegsol-sat11-strips', 'philosophers',
+    'pipesworld-notankage', 'pipesworld-tankage', 'psr-large',
+    'psr-middle', 'psr-small', 'rovers', 'satellite',
+    'scanalyzer-08-strips', 'scanalyzer-sat11-strips', 'schedule',
+    'settlers-sat18-adl', 'snake-sat18-strips', 'sokoban-sat08-strips',
+    'sokoban-sat11-strips', 'spider-sat18-strips', 'storage',
+    'termes-sat18-strips', 'tetris-sat14-strips',
+    'thoughtful-sat14-strips', 'tidybot-sat11-strips', 'tpp',
+    'transport-sat08-strips', 'transport-sat11-strips',
+    'transport-sat14-strips', 'trucks', 'trucks-strips',
+    'visitall-sat11-strips', 'visitall-sat14-strips',
+    'woodworking-sat08-strips', 'woodworking-sat11-strips',
+    'zenotravel']
+
+
+def get_script():
+    """Get file name of main script."""
+    return tools.get_script_path()
+
+
+def get_script_dir():
+    """Get directory of main script.
+
+    Usually a relative directory (depends on how it was called by the user.)"""
+    return os.path.dirname(get_script())
+
+
+def get_experiment_name():
+    """Get name for experiment.
+
+    Derived from the absolute filename of the main script, e.g.
+    "/ham/spam/eggs.py" => "spam-eggs"."""
+    script = os.path.abspath(get_script())
+    script_dir = os.path.basename(os.path.dirname(script))
+    script_base = os.path.splitext(os.path.basename(script))[0]
+    return "%s-%s" % (script_dir, script_base)
+
+
+def get_data_dir():
+    """Get data dir for the experiment.
+
+    This is the subdirectory "data" of the directory containing
+    the main script."""
+    return os.path.join(get_script_dir(), "data", get_experiment_name())
+
+
+def get_repo_base():
+    """Get base directory of the repository, as an absolute path.
+
+    Search upwards in the directory tree from the main script until a
+    directory with a subdirectory named ".hg" is found.
+
+    Abort if the repo base cannot be found."""
+    path = os.path.abspath(get_script_dir())
+    while os.path.dirname(path) != path:
+        if os.path.exists(os.path.join(path, ".hg")):
+            return path
+        path = os.path.dirname(path)
+    sys.exit("repo base could not be found")
+
+
+def is_running_on_cluster():
+    node = platform.node()
+    return node.endswith(".scicore.unibas.ch") or node.endswith(".cluster.bc2.ch")
+
+
+def is_test_run():
+    return ARGS.test_run == "yes" or (
+        ARGS.test_run == "auto" and not is_running_on_cluster())
+
+
+def get_algo_nick(revision, config_nick):
+    return "{revision}-{config_nick}".format(**locals())
+
+
+class IssueConfig(object):
+    """Hold information about a planner configuration.
+
+    See FastDownwardExperiment.add_algorithm() for documentation of the
+    constructor's options.
+
+    """
+    def __init__(self, nick, component_options,
+                 build_options=None, driver_options=None):
+        self.nick = nick
+        self.component_options = component_options
+        self.build_options = build_options
+        self.driver_options = driver_options
+
+
+class IssueExperiment(FastDownwardExperiment):
+    """Subclass of FastDownwardExperiment with some convenience features."""
+
+    DEFAULT_TEST_SUITE = ["depot:p01.pddl", "gripper:prob01.pddl"]
+
+    DEFAULT_TABLE_ATTRIBUTES = [
+        "cost",
+        "coverage",
+        "error",
+        "evaluations",
+        "expansions",
+        "expansions_until_last_jump",
+        "generated",
+        "memory",
+        "planner_memory",
+        "planner_time",
+        "quality",
+        "run_dir",
+        "score_evaluations",
+        "score_expansions",
+        "score_generated",
+        "score_memory",
+        "score_search_time",
+        "score_total_time",
+        "search_time",
+        "total_time",
+        ]
+
+    DEFAULT_SCATTER_PLOT_ATTRIBUTES = [
+        "evaluations",
+        "expansions",
+        "expansions_until_last_jump",
+        "initial_h_value",
+        "memory",
+        "search_time",
+        "total_time",
+        ]
+
+    PORTFOLIO_ATTRIBUTES = [
+        "cost",
+        "coverage",
+        "error",
+        "plan_length",
+        "run_dir",
+        ]
+
+    def __init__(self, revisions=None, configs=None, path=None, **kwargs):
+        """
+
+        You can either specify both *revisions* and *configs* or none
+        of them. If they are omitted, you will need to call
+        exp.add_algorithm() manually.
+
+        If *revisions* is given, it must be a non-empty list of
+        revision identifiers, which specify which planner versions to
+        use in the experiment. The same versions are used for
+        translator, preprocessor and search. ::
+
+            IssueExperiment(revisions=["issue123", "4b3d581643"], ...)
+
+        If *configs* is given, it must be a non-empty list of
+        IssueConfig objects. ::
+
+            IssueExperiment(..., configs=[
+                IssueConfig("ff", ["--search", "eager_greedy(ff())"]),
+                IssueConfig(
+                    "lama", [],
+                    driver_options=["--alias", "seq-sat-lama-2011"]),
+            ])
+
+        If *path* is specified, it must be the path to where the
+        experiment should be built (e.g.
+        /home/john/experiments/issue123/exp01/). If omitted, the
+        experiment path is derived automatically from the main
+        script's filename. Example::
+
+            script = experiments/issue123/exp01.py -->
+            path = experiments/issue123/data/issue123-exp01/
+
+        """
+
+        path = path or get_data_dir()
+
+        FastDownwardExperiment.__init__(self, path=path, **kwargs)
+
+        if (revisions and not configs) or (not revisions and configs):
+            raise ValueError(
+                "please provide either both or none of revisions and configs")
+
+        for rev in revisions:
+            for config in configs:
+                self.add_algorithm(
+                    get_algo_nick(rev, config.nick),
+                    get_repo_base(),
+                    rev,
+                    config.component_options,
+                    build_options=config.build_options,
+                    driver_options=config.driver_options)
+
+        self._revisions = revisions
+        self._configs = configs
+
+    @classmethod
+    def _is_portfolio(cls, config_nick):
+        return "fdss" in config_nick
+
+    @classmethod
+    def get_supported_attributes(cls, config_nick, attributes):
+        if cls._is_portfolio(config_nick):
+            return [attr for attr in attributes
+                    if attr in cls.PORTFOLIO_ATTRIBUTES]
+        return attributes
+
+    def add_absolute_report_step(self, **kwargs):
+        """Add step that makes an absolute report.
+
+        Absolute reports are useful for experiments that don't compare
+        revisions.
+
+        The report is written to the experiment evaluation directory.
+
+        All *kwargs* will be passed to the AbsoluteReport class. If the
+        keyword argument *attributes* is not specified, a default list
+        of attributes is used. ::
+
+            exp.add_absolute_report_step(attributes=["coverage"])
+
+        """
+        kwargs.setdefault("attributes", self.DEFAULT_TABLE_ATTRIBUTES)
+        report = AbsoluteReport(**kwargs)
+        outfile = os.path.join(
+            self.eval_dir,
+            get_experiment_name() + "." + report.output_format)
+        self.add_report(report, outfile=outfile)
+        self.add_step(
+            'publish-absolute-report', subprocess.call, ['publish', outfile])
+
+    def add_comparison_table_step(self, **kwargs):
+        """Add a step that makes pairwise revision comparisons.
+
+        Create comparative reports for all pairs of Fast Downward
+        revisions. Each report pairs up the runs of the same config and
+        lists the two absolute attribute values and their difference
+        for all attributes in kwargs["attributes"].
+
+        All *kwargs* will be passed to the CompareConfigsReport class.
+        If the keyword argument *attributes* is not specified, a
+        default list of attributes is used. ::
+
+            exp.add_comparison_table_step(attributes=["coverage"])
+
+        """
+        kwargs.setdefault("attributes", self.DEFAULT_TABLE_ATTRIBUTES)
+
+        def make_comparison_tables():
+            for rev1, rev2 in itertools.combinations(self._revisions, 2):
+                compared_configs = []
+                for config in self._configs:
+                    config_nick = config.nick
+                    compared_configs.append(
+                        ("%s-%s" % (rev1, config_nick),
+                         "%s-%s" % (rev2, config_nick),
+                         "Diff (%s)" % config_nick))
+                report = ComparativeReport(compared_configs, **kwargs)
+                outfile = os.path.join(
+                    self.eval_dir,
+                    "%s-%s-%s-compare.%s" % (
+                        self.name, rev1, rev2, report.output_format))
+                report(self.eval_dir, outfile)
+
+        def publish_comparison_tables():
+            for rev1, rev2 in itertools.combinations(self._revisions, 2):
+                outfile = os.path.join(
+                    self.eval_dir,
+                    "%s-%s-%s-compare.html" % (self.name, rev1, rev2))
+                subprocess.call(["publish", outfile])
+
+        self.add_step("make-comparison-tables", make_comparison_tables)
+        self.add_step(
+            "publish-comparison-tables", publish_comparison_tables)
+
+    def add_scatter_plot_step(self, relative=False, attributes=None):
+        """Add step creating (relative) scatter plots for all revision pairs.
+
+        Create a scatter plot for each combination of attribute,
+        configuration and revisions pair. If *attributes* is not
+        specified, a list of common scatter plot attributes is used.
+        For portfolios all attributes except "cost", "coverage" and
+        "plan_length" will be ignored. ::
+
+            exp.add_scatter_plot_step(attributes=["expansions"])
+
+        """
+        if relative:
+            report_class = RelativeScatterPlotReport
+            scatter_dir = os.path.join(self.eval_dir, "scatter-relative")
+            step_name = "make-relative-scatter-plots"
+        else:
+            report_class = ScatterPlotReport
+            scatter_dir = os.path.join(self.eval_dir, "scatter-absolute")
+            step_name = "make-absolute-scatter-plots"
+        if attributes is None:
+            attributes = self.DEFAULT_SCATTER_PLOT_ATTRIBUTES
+
+        def make_scatter_plot(config_nick, rev1, rev2, attribute):
+            name = "-".join([self.name, rev1, rev2, attribute, config_nick])
+            print "Make scatter plot for", name
+            algo1 = "{}-{}".format(rev1, config_nick)
+            algo2 = "{}-{}".format(rev2, config_nick)
+            report = report_class(
+                filter_algorithm=[algo1, algo2],
+                attributes=[attribute],
+                get_category=lambda run1, run2: run1["domain"],
+                # legend_location=(1.3, 0.5),
+                )
+            report(
+                self.eval_dir,
+                os.path.join(scatter_dir, rev1 + "-" + rev2, name))
+
+        def make_scatter_plots():
+            for config in self._configs:
+                for rev1, rev2 in itertools.combinations(self._revisions, 2):
+                    for attribute in self.get_supported_attributes(
+                            config.nick, attributes):
+                        make_scatter_plot(config.nick, rev1, rev2, attribute)
+
+        self.add_step(step_name, make_scatter_plots)
diff -X lp-recognizer/fd-patch.ignore -ruN fast-downward-original/experiments/issue851/generalscatter.py fast-downward/experiments/issue851/generalscatter.py
--- fast-downward-original/experiments/issue851/generalscatter.py	1969-12-31 21:00:00.000000000 -0300
+++ fast-downward/experiments/issue851/generalscatter.py	2019-02-11 14:13:33.000000000 -0200
@@ -0,0 +1,305 @@
+#! /usr/bin/env python
+# -*- coding: utf-8 -*-
+
+from collections import defaultdict
+import logging
+import math
+import os
+
+from lab import tools
+
+from downward.reports.plot import MatplotlibPlot, Matplotlib, PgfPlots, \
+    PlotReport, MIN_AXIS
+
+
+class ScatterMatplotlib(Matplotlib):
+    @classmethod
+    def _plot(cls, report, axes, categories, styles):
+        # Display grid
+        axes.grid(b=True, linestyle='-', color='0.75')
+
+        has_points = False
+        # Generate the scatter plots
+        for category, coords in sorted(categories.items()):
+            X, Y = zip(*coords)
+            axes.scatter(X, Y, s=42, label=category, **styles[category])
+            if X and Y:
+                has_points = True
+
+        if report.xscale == 'linear' or report.yscale == 'linear':
+            # TODO: assert that both are linear or log
+            plot_size = max(report.x_missing_val * 1.01, report.y_missing_val * 1.01)
+        else:
+            plot_size = max(report.x_missing_val * 1.5, report.y_missing_val * 1.5)
+
+        # Plot a diagonal black line. Starting at (0,0) often raises errors.
+        axes.plot([0.001, plot_size], [0.001, plot_size], 'k')
+
+        axes.set_xlim(report.xlim_left or -1, report.xlim_right or plot_size)
+        axes.set_ylim(report.ylim_bottom or -1, report.ylim_top or plot_size)
+        # axes.set_xlim(report.xlim_left, report.xlim_right)
+        # axes.set_ylim(report.ylim_bottom, report.ylim_top)
+
+        for axis in [axes.xaxis, axes.yaxis]:
+            # MatplotlibPlot.change_axis_formatter(
+                # axis, report.missing_val if report.show_missing else None)
+            MatplotlibPlot.change_axis_formatter(axes.xaxis,
+                report.x_missing_val if report.show_missing else None)
+            MatplotlibPlot.change_axis_formatter(axes.yaxis,
+                report.y_missing_val if report.show_missing else None)
+        return has_points
+
+
+class ScatterPgfPlots(PgfPlots):
+    @classmethod
+    def _format_coord(cls, coord):
+        def format_value(v):
+            return str(v) if isinstance(v, int) else '%f' % v
+        return '(%s, %s)' % (format_value(coord[0]), format_value(coord[1]))
+
+    @classmethod
+    def _get_plot(cls, report):
+        lines = []
+        options = cls._get_axis_options(report)
+        lines.append('\\begin{axis}[%s]' % cls._format_options(options))
+        for category, coords in sorted(report.categories.items()):
+            plot = {'only marks': True}
+            lines.append(
+                '\\addplot+[%s] coordinates {\n%s\n};' % (
+                    cls._format_options(plot),
+                    ' '.join(cls._format_coord(c) for c in coords)))
+            if category:
+                lines.append('\\addlegendentry{%s}' % category)
+            elif report.has_multiple_categories:
+                # None is treated as the default category if using multiple
+                # categories. Add a corresponding entry to the legend.
+                lines.append('\\addlegendentry{default}')
+        # Add black line.
+        start = min(report.min_x, report.min_y)
+        if report.xlim_left is not None:
+            start = min(start, report.xlim_left)
+        if report.ylim_bottom is not None:
+            start = min(start, report.ylim_bottom)
+        end = max(report.max_x, report.max_y)
+        if report.xlim_right:
+            end = max(end, report.xlim_right)
+        if report.ylim_top:
+            end = max(end, report.ylim_top)
+        if report.show_missing:
+            end = max(end, report.missing_val)
+        lines.append(
+            '\\addplot[color=black] coordinates {(%f, %f) (%d, %d)};' %
+            (start, start, end, end))
+        lines.append('\\end{axis}')
+        return lines
+
+    @classmethod
+    def _get_axis_options(cls, report):
+        opts = PgfPlots._get_axis_options(report)
+        # Add line for missing values.
+        for axis in ['x', 'y']:
+            opts['extra %s ticks' % axis] = report.missing_val
+            opts['extra %s tick style' % axis] = 'grid=major'
+        return opts
+
+class GeneralScatterPlotReport(PlotReport):
+    """
+    Generate a scatter plot for a specific attribute.
+    """
+    def __init__(self, x_algo, y_algo, x_attribute, y_attribute, show_missing=True, get_category=None, **kwargs):
+        """
+        See :class:`.PlotReport` for inherited arguments.
+
+        The keyword argument *attributes* must contain exactly one
+        attribute.
+
+        Use the *filter_algorithm* keyword argument to select exactly
+        two algorithms.
+
+        If only one of the two algorithms has a value for a run, only
+        add a coordinate if *show_missing* is True.
+
+        *get_category* can be a function that takes **two** runs
+        (dictionaries of properties) and returns a category name. This
+        name is used to group the points in the plot. If there is more
+        than one group, a legend is automatically added. Runs for which
+        this function returns None are shown in a default category and
+        are not contained in the legend. For example, to group by
+        domain:
+
+        >>> def domain_as_category(run1, run2):
+        ...     # run2['domain'] has the same value, because we always
+        ...     # compare two runs of the same problem.
+        ...     return run1['domain']
+
+        Example grouping by difficulty:
+
+        >>> def improvement(run1, run2):
+        ...     time1 = run1.get('search_time', 1800)
+        ...     time2 = run2.get('search_time', 1800)
+        ...     if time1 > time2:
+        ...         return 'better'
+        ...     if time1 == time2:
+        ...         return 'equal'
+        ...     return 'worse'
+
+        >>> from downward.experiment import FastDownwardExperiment
+        >>> exp = FastDownwardExperiment()
+        >>> exp.add_report(ScatterPlotReport(
+        ...     attributes=['search_time'],
+        ...     get_category=improvement))
+
+        Example comparing the number of expanded states for two
+        algorithms:
+
+        >>> exp.add_report(ScatterPlotReport(
+        ...         attributes=["expansions_until_last_jump"],
+        ...         filter_algorithm=["algorithm-1", "algorithm-2"],
+        ...         get_category=domain_as_category,
+        ...         format="png",  # Use "tex" for pgfplots output.
+        ...         ),
+        ...     name="scatterplot-expansions")
+
+        """
+        # If the size has not been set explicitly, make it a square.
+        matplotlib_options = kwargs.get('matplotlib_options', {})
+        matplotlib_options.setdefault('figure.figsize', [8, 8])
+        kwargs['matplotlib_options'] = matplotlib_options
+        PlotReport.__init__(self, **kwargs)
+        if not self.attribute:
+            logging.critical('ScatterPlotReport needs exactly one attribute')
+        # By default all values are in the same category.
+        self.get_category = get_category or (lambda run1, run2: None)
+        self.show_missing = show_missing
+        self.xlim_left = self.xlim_left or MIN_AXIS
+        self.ylim_bottom = self.ylim_bottom or MIN_AXIS
+        if self.output_format == 'tex':
+            self.writer = ScatterPgfPlots
+        else:
+            self.writer = ScatterMatplotlib
+        self.x_algo = x_algo
+        self.y_algo = y_algo
+        self.x_attribute = x_attribute
+        self.y_attribute = y_attribute
+
+    def _set_scales(self, xscale, yscale):
+        PlotReport._set_scales(self, xscale or self.attribute.scale or 'log', yscale)
+        if self.xscale != self.yscale:
+            logging.critical('Scatterplots must use the same scale on both axes.')
+
+    def _get_missing_val(self, max_value, scale):
+        """
+        Separate the missing values by plotting them at (max_value * 10)
+        rounded to the next power of 10.
+        """
+        assert max_value is not None
+        # HACK!
+        max_value = 1800
+        if scale == 'linear':
+            return max_value * 1.1
+        return int(10 ** math.ceil(math.log10(max_value)))
+
+    def _handle_none_values(self, X, Y, replacement_x, replacement_y):
+        assert len(X) == len(Y), (X, Y)
+        if self.show_missing:
+            return ([x if x is not None else replacement_x for x in X],
+                    [y if y is not None else replacement_y for y in Y])
+        return zip(*[(x, y) for x, y in zip(X, Y) if x is not None and y is not None])
+
+    def _fill_categories(self, runs):
+        # We discard the *runs* parameter.
+        # Map category names to value tuples
+        categories = defaultdict(list)
+        x_count = 0
+        y_count = 0
+        x_none_count = 0
+        y_none_count = 0
+        for (domain, problem), runs in self.problem_runs.items():
+            run1 = next((run for run in runs if run['algorithm'] == self.x_algo), None)
+            run2 = next((run for run in runs if run['algorithm'] == self.y_algo), None)
+            if run1 is None or run2 is None:
+                continue
+            assert (run1['algorithm'] == self.x_algo and
+                    run2['algorithm'] == self.y_algo)
+            val1 = run1.get(self.x_attribute)
+            val2 = run2.get(self.y_attribute)
+            x_count += 1
+            y_count += 1
+            if val1 is None:
+                x_none_count += 1
+            if val2 is None:
+                y_none_count += 1
+            # print val1, val2
+            if val1 is None and val2 is None:
+                continue
+            category = self.get_category(run1, run2)
+            categories[category].append((val1, val2))
+        # print x_count, y_count
+        # print x_none_count, y_none_count
+        # print len(categories[None])
+        # print categories[None]
+        return categories
+
+    def _get_limit(self, varlist, limit_type):
+        assert limit_type == 'max' or limit_type == 'min'
+        varlist = [x for x in varlist if x is not None]
+        if(limit_type == 'max'):
+            return max(varlist)
+        else:
+            return min(varlist)
+
+    def _get_plot_size(self, missing_val, scale):
+        if scale == 'linear':
+            return missing_val * 1.01
+        else:
+            return missing_val * 1.25
+
+    def _prepare_categories(self, categories):
+        categories = PlotReport._prepare_categories(self, categories)
+
+        # Find max-value to fit plot and to draw missing values.
+        # self.missing_val = self._get_missing_val(max(self.max_x, self.max_y))
+        self.x_missing_val = self._get_missing_val(self.max_x, self.xscale)
+        self.y_missing_val = self._get_missing_val(self.max_y, self.yscale)
+        # print self.x_missing_val, self.y_missing_val
+
+        # set minima
+        self.xlim_left = self._get_limit([self.xlim_left, self.min_x],'min')
+        self.ylim_bottom = self._get_limit([self.ylim_bottom, self.min_y],'min')
+
+        # set maxima
+        x_plot_size = y_plot_size = None
+        if self.show_missing:
+            x_plot_size = self._get_plot_size(self.x_missing_val, self.xscale)
+            y_plot_size = self._get_plot_size(self.y_missing_val, self.yscale)
+        self.xlim_right = self._get_limit([self.xlim_right, self.max_x, x_plot_size], 'max')
+        self.ylim_top = self._get_limit([self.ylim_top, self.max_y, y_plot_size], 'max')
+
+        # self.diagonal_start = self.diagonal_end = None
+        # if self.show_diagonal:
+            # self.diagonal_start = max(self.xlim_left, self.ylim_bottom)
+            # self.diagonal_end = min(self.xlim_right, self.ylim_top)
+
+        new_categories = {}
+        for category, coords in categories.items():
+            X, Y = zip(*coords)
+            # X, Y = self._handle_none_values(X, Y, self.missing_val)
+            X, Y = self._handle_none_values(X, Y, self.x_missing_val, self.y_missing_val)
+            coords = zip(X, Y)
+            new_categories[category] = coords
+        # print len(new_categories[None])
+        # print new_categories[None]
+        return new_categories
+
+    def write(self):
+        if not (len(self.algorithms) == 1 and self.x_algo == self.algorithms[0] and self.y_algo == self.algorithms[0]):
+            logging.critical(
+                'Scatter plots need exactly 1 algorithm that must match x_algo and y_algo: %s, %s, %s' % (self.algorithms, self.x_algo, self.y_algo))
+        self.xlabel = self.xlabel or self.x_algo + ": " + self.x_attribute
+        self.ylabel = self.ylabel or self.y_algo + ": " + self.y_attribute
+
+        suffix = '.' + self.output_format
+        if not self.outfile.endswith(suffix):
+            self.outfile += suffix
+        tools.makedirs(os.path.dirname(self.outfile))
+        self._write_plot(self.runs.values(), self.outfile)
diff -X lp-recognizer/fd-patch.ignore -ruN fast-downward-original/experiments/issue851/ms-parser.py fast-downward/experiments/issue851/ms-parser.py
--- fast-downward-original/experiments/issue851/ms-parser.py	1969-12-31 21:00:00.000000000 -0300
+++ fast-downward/experiments/issue851/ms-parser.py	2019-02-11 14:13:33.000000000 -0200
@@ -0,0 +1,70 @@
+#! /usr/bin/env python
+
+from lab.parser import Parser
+
+parser = Parser()
+parser.add_pattern('ms_final_size', 'Final transition system size: (\d+)', required=False, type=int)
+parser.add_pattern('ms_construction_time', 'Merge-and-shrink algorithm runtime: (.+)s', required=False, type=float)
+parser.add_pattern('ms_atomic_construction_time', 't=(.+)s \(after computation of atomic transition systems\)', required=False, type=float)
+parser.add_pattern('ms_memory_delta', 'Final peak memory increase of merge-and-shrink computation: (\d+) KB', required=False, type=int)
+
+def check_ms_constructed(content, props):
+    ms_construction_time = props.get('ms_construction_time')
+    abstraction_constructed = False
+    if ms_construction_time is not None:
+        abstraction_constructed = True
+    props['ms_abstraction_constructed'] = abstraction_constructed
+
+parser.add_function(check_ms_constructed)
+
+def check_atomic_fts_constructed(content, props):
+    ms_atomic_construction_time = props.get('ms_atomic_construction_time')
+    ms_atomic_fts_constructed = False
+    if ms_atomic_construction_time is not None:
+        ms_atomic_fts_constructed = True
+    props['ms_atomic_fts_constructed'] = ms_atomic_fts_constructed
+
+parser.add_function(check_atomic_fts_constructed)
+
+def check_planner_exit_reason(content, props):
+    ms_abstraction_constructed = props.get('ms_abstraction_constructed')
+    error = props.get('error')
+    if error != 'success' and error != 'timeout' and error != 'out-of-memory':
+        print 'error: %s' % error
+        return
+
+    # Check whether merge-and-shrink computation or search ran out of
+    # time or memory.
+    ms_out_of_time = False
+    ms_out_of_memory = False
+    search_out_of_time = False
+    search_out_of_memory = False
+    if ms_abstraction_constructed == False:
+        if error == 'timeout':
+            ms_out_of_time = True
+        elif error == 'out-of-memory':
+            ms_out_of_memory = True
+    elif ms_abstraction_constructed == True:
+        if error == 'timeout':
+            search_out_of_time = True
+        elif error == 'out-of-memory':
+            search_out_of_memory = True
+    props['ms_out_of_time'] = ms_out_of_time
+    props['ms_out_of_memory'] = ms_out_of_memory
+    props['search_out_of_time'] = search_out_of_time
+    props['search_out_of_memory'] = search_out_of_memory
+
+parser.add_function(check_planner_exit_reason)
+
+def check_perfect_heuristic(content, props):
+    plan_length = props.get('plan_length')
+    expansions = props.get('expansions')
+    if plan_length != None:
+        perfect_heuristic = False
+        if plan_length + 1 == expansions:
+            perfect_heuristic = True
+        props['perfect_heuristic'] = perfect_heuristic
+
+parser.add_function(check_perfect_heuristic)
+
+parser.parse()
diff -X lp-recognizer/fd-patch.ignore -ruN fast-downward-original/experiments/issue851/relativescatter.py fast-downward/experiments/issue851/relativescatter.py
--- fast-downward-original/experiments/issue851/relativescatter.py	1969-12-31 21:00:00.000000000 -0300
+++ fast-downward/experiments/issue851/relativescatter.py	2019-02-11 14:13:33.000000000 -0200
@@ -0,0 +1,105 @@
+# -*- coding: utf-8 -*-
+
+from collections import defaultdict
+
+from matplotlib import ticker
+
+from downward.reports.scatter import ScatterPlotReport
+from downward.reports.plot import PlotReport, Matplotlib, MatplotlibPlot
+
+
+# TODO: handle outliers
+
+# TODO: this is mostly copied from ScatterMatplotlib (scatter.py)
+class RelativeScatterMatplotlib(Matplotlib):
+    @classmethod
+    def _plot(cls, report, axes, categories, styles):
+        # Display grid
+        axes.grid(b=True, linestyle='-', color='0.75')
+
+        has_points = False
+        # Generate the scatter plots
+        for category, coords in sorted(categories.items()):
+            X, Y = zip(*coords)
+            axes.scatter(X, Y, s=42, label=category, **styles[category])
+            if X and Y:
+                has_points = True
+
+        if report.xscale == 'linear' or report.yscale == 'linear':
+            plot_size = report.missing_val * 1.01
+        else:
+            plot_size = report.missing_val * 1.25
+
+        # make 5 ticks above and below 1
+        yticks = []
+        tick_step = report.ylim_top**(1/5.0)
+        for i in xrange(-5, 6):
+            yticks.append(tick_step**i)
+        axes.set_yticks(yticks)
+        axes.get_yaxis().set_major_formatter(ticker.ScalarFormatter())
+
+        axes.set_xlim(report.xlim_left or -1, report.xlim_right or plot_size)
+        axes.set_ylim(report.ylim_bottom or -1, report.ylim_top or plot_size)
+
+        for axis in [axes.xaxis, axes.yaxis]:
+            MatplotlibPlot.change_axis_formatter(
+                axis,
+                report.missing_val if report.show_missing else None)
+        return has_points
+
+
+class RelativeScatterPlotReport(ScatterPlotReport):
+    """
+    Generate a scatter plot that shows a relative comparison of two
+    algorithms with regard to the given attribute. The attribute value
+    of algorithm 1 is shown on the x-axis and the relation to the value
+    of algorithm 2 on the y-axis.
+    """
+
+    def __init__(self, show_missing=True, get_category=None, **kwargs):
+        ScatterPlotReport.__init__(self, show_missing, get_category, **kwargs)
+        if self.output_format == 'tex':
+            raise "not supported"
+        else:
+            self.writer = RelativeScatterMatplotlib
+
+    def _fill_categories(self, runs):
+        # We discard the *runs* parameter.
+        # Map category names to value tuples
+        categories = defaultdict(list)
+        self.ylim_bottom = 2
+        self.ylim_top = 0.5
+        self.xlim_left = float("inf")
+        for (domain, problem), runs in self.problem_runs.items():
+            if len(runs) != 2:
+                continue
+            run1, run2 = runs
+            assert (run1['algorithm'] == self.algorithms[0] and
+                    run2['algorithm'] == self.algorithms[1])
+            val1 = run1.get(self.attribute)
+            val2 = run2.get(self.attribute)
+            if val1 is None or val2 is None:
+                continue
+            category = self.get_category(run1, run2)
+            assert val1 > 0, (domain, problem, self.algorithms[0], val1)
+            assert val2 > 0, (domain, problem, self.algorithms[1], val2)
+            x = val1
+            y = val2 / float(val1)
+
+            categories[category].append((x, y))
+
+            self.ylim_top = max(self.ylim_top, y)
+            self.ylim_bottom = min(self.ylim_bottom, y)
+            self.xlim_left = min(self.xlim_left, x)
+
+        # center around 1
+        if self.ylim_bottom < 1:
+            self.ylim_top = max(self.ylim_top, 1 / float(self.ylim_bottom))
+        if self.ylim_top > 1:
+            self.ylim_bottom = min(self.ylim_bottom, 1 / float(self.ylim_top))
+        return categories
+
+    def _set_scales(self, xscale, yscale):
+        # ScatterPlot uses log-scaling on the x-axis by default.
+        PlotReport._set_scales(
+            self, xscale or self.attribute.scale or 'log', 'log')
diff -X lp-recognizer/fd-patch.ignore -ruN fast-downward-original/experiments/issue851/v1.py fast-downward/experiments/issue851/v1.py
--- fast-downward-original/experiments/issue851/v1.py	1969-12-31 21:00:00.000000000 -0300
+++ fast-downward/experiments/issue851/v1.py	2019-02-11 14:13:33.000000000 -0200
@@ -0,0 +1,122 @@
+#! /usr/bin/env python
+# -*- coding: utf-8 -*-
+
+import itertools
+import os
+
+from lab.environments import LocalEnvironment, BaselSlurmEnvironment
+from lab.reports import Attribute, geometric_mean
+
+from downward.reports.compare import ComparativeReport
+
+import common_setup
+from common_setup import IssueConfig, IssueExperiment
+from generalscatter import GeneralScatterPlotReport
+from relativescatter import RelativeScatterPlotReport
+
+DIR = os.path.dirname(os.path.abspath(__file__))
+SCRIPT_NAME = os.path.splitext(os.path.basename(__file__))[0]
+BENCHMARKS_DIR = os.environ["DOWNWARD_BENCHMARKS"]
+REVISIONS = ["issue851-base", "issue851-v1"]
+BUILDS = ["release32"]
+CONFIG_NICKS = [
+    ('dfp-b50k', ['--search', 'astar(merge_and_shrink(merge_strategy=merge_stateless(merge_selector=score_based_filtering(scoring_functions=[goal_relevance,dfp,total_order])),shrink_strategy=shrink_bisimulation(greedy=false),label_reduction=exact(before_shrinking=true,before_merging=false),max_states=50000,threshold_before_merge=1))']),
+    ('rl-b50k', ['--search', 'astar(merge_and_shrink(merge_strategy=merge_precomputed(merge_tree=linear(variable_order=reverse_level)),shrink_strategy=shrink_bisimulation(greedy=false),label_reduction=exact(before_shrinking=true,before_merging=false),max_states=50000,threshold_before_merge=1))']),
+    ('sccs-dfp-b50k', ['--search', 'astar(merge_and_shrink(shrink_strategy=shrink_bisimulation(greedy=false),merge_strategy=merge_sccs(order_of_sccs=topological,merge_selector=score_based_filtering(scoring_functions=[goal_relevance,dfp,total_order(atomic_ts_order=reverse_level,product_ts_order=new_to_old,atomic_before_product=false)])),label_reduction=exact(before_shrinking=true,before_merging=false),max_states=50000,threshold_before_merge=1))']),
+]
+CONFIGS = [
+    IssueConfig(
+        config_nick,
+        config,
+        build_options=[build],
+        driver_options=["--build", build])
+    for build in BUILDS
+    for config_nick, config in CONFIG_NICKS
+]
+
+SUITE = common_setup.DEFAULT_OPTIMAL_SUITE
+ENVIRONMENT = BaselSlurmEnvironment(
+    partition="infai_2",
+    email="silvan.sievers@unibas.ch",
+    export=["PATH", "DOWNWARD_BENCHMARKS"])
+
+if common_setup.is_test_run():
+    SUITE = IssueExperiment.DEFAULT_TEST_SUITE
+    ENVIRONMENT = LocalEnvironment(processes=4)
+
+exp = IssueExperiment(
+    revisions=REVISIONS,
+    configs=CONFIGS,
+    environment=ENVIRONMENT,
+)
+exp.add_suite(BENCHMARKS_DIR, SUITE)
+
+exp.add_parser(exp.EXITCODE_PARSER)
+exp.add_parser(exp.TRANSLATOR_PARSER)
+exp.add_parser(exp.SINGLE_SEARCH_PARSER)
+exp.add_parser(exp.PLANNER_PARSER)
+exp.add_parser('ms-parser.py')
+
+exp.add_step('build', exp.build)
+exp.add_step('start', exp.start_runs)
+exp.add_fetcher(name='fetch')
+
+# planner outcome attributes
+perfect_heuristic = Attribute('perfect_heuristic', absolute=True, min_wins=False)
+
+# m&s attributes
+ms_construction_time = Attribute('ms_construction_time', absolute=False, min_wins=True, functions=[geometric_mean])
+ms_atomic_construction_time = Attribute('ms_atomic_construction_time', absolute=False, min_wins=True, functions=[geometric_mean])
+ms_abstraction_constructed = Attribute('ms_abstraction_constructed', absolute=True, min_wins=False)
+ms_atomic_fts_constructed = Attribute('ms_atomic_fts_constructed', absolute=True, min_wins=False)
+ms_final_size = Attribute('ms_final_size', absolute=False, min_wins=True)
+ms_out_of_memory = Attribute('ms_out_of_memory', absolute=True, min_wins=True)
+ms_out_of_time = Attribute('ms_out_of_time', absolute=True, min_wins=True)
+search_out_of_memory = Attribute('search_out_of_memory', absolute=True, min_wins=True)
+search_out_of_time = Attribute('search_out_of_time', absolute=True, min_wins=True)
+
+extra_attributes = [
+    perfect_heuristic,
+
+    ms_construction_time,
+    ms_atomic_construction_time,
+    ms_abstraction_constructed,
+    ms_atomic_fts_constructed,
+    ms_final_size,
+    ms_out_of_memory,
+    ms_out_of_time,
+    search_out_of_memory,
+    search_out_of_time,
+]
+attributes = exp.DEFAULT_TABLE_ATTRIBUTES
+attributes.extend(extra_attributes)
+
+# TODO: remove this filter when re-running experiments
+def check_atomic_fts_constructed(run):
+    ms_atomic_construction_time = run.get('ms_atomic_construction_time')
+    ms_atomic_fts_constructed = False
+    if ms_atomic_construction_time is not None:
+        ms_atomic_fts_constructed = True
+    run['ms_atomic_fts_constructed'] = ms_atomic_fts_constructed
+    return run
+
+exp.add_comparison_table_step(attributes=attributes,filter=[check_atomic_fts_constructed])
+
+exp.add_scatter_plot_step(attributes=[ms_atomic_construction_time])
+
+for algo_nick in ['dfp-b50k', 'rl-b50k', 'sccs-dfp-b50k']:
+    algo = "issue851-v1-{}".format(algo_nick)
+    exp.add_report(
+        GeneralScatterPlotReport(
+            x_algo = algo,
+            y_algo = algo,
+            x_attribute='ms_atomic_construction_time',
+            y_attribute='total_time',
+            filter_algorithm=[algo],
+            attributes=['total_time'],
+            get_category=lambda run1, run2: run1["domain"],
+        ),
+        outfile='{}-total_time_vs_ms_atomic_construction_time.png'.format(algo),
+    )
+
+exp.run_steps()
diff -X lp-recognizer/fd-patch.ignore -ruN fast-downward-original/experiments/issue851/v2.py fast-downward/experiments/issue851/v2.py
--- fast-downward-original/experiments/issue851/v2.py	1969-12-31 21:00:00.000000000 -0300
+++ fast-downward/experiments/issue851/v2.py	2019-02-11 14:13:33.000000000 -0200
@@ -0,0 +1,114 @@
+#! /usr/bin/env python
+# -*- coding: utf-8 -*-
+
+import itertools
+import os
+
+from lab.environments import LocalEnvironment, BaselSlurmEnvironment
+from lab.reports import Attribute, geometric_mean
+
+from downward.reports.compare import ComparativeReport
+
+import common_setup
+from common_setup import IssueConfig, IssueExperiment
+from generalscatter import GeneralScatterPlotReport
+from relativescatter import RelativeScatterPlotReport
+
+DIR = os.path.dirname(os.path.abspath(__file__))
+SCRIPT_NAME = os.path.splitext(os.path.basename(__file__))[0]
+BENCHMARKS_DIR = os.environ["DOWNWARD_BENCHMARKS"]
+REVISIONS = ["issue851-base-v2", "issue851-v2"]
+BUILDS = ["release32"]
+CONFIG_NICKS = [
+    ('dfp-b50k', ['--search', 'astar(merge_and_shrink(merge_strategy=merge_stateless(merge_selector=score_based_filtering(scoring_functions=[goal_relevance,dfp,total_order])),shrink_strategy=shrink_bisimulation(greedy=false),label_reduction=exact(before_shrinking=true,before_merging=false),max_states=50000,threshold_before_merge=1))']),
+    # ('rl-b50k', ['--search', 'astar(merge_and_shrink(merge_strategy=merge_precomputed(merge_tree=linear(variable_order=reverse_level)),shrink_strategy=shrink_bisimulation(greedy=false),label_reduction=exact(before_shrinking=true,before_merging=false),max_states=50000,threshold_before_merge=1))']),
+    # ('sbmiasm-b50k', ['--search', 'astar(merge_and_shrink(shrink_strategy=shrink_bisimulation(greedy=false),merge_strategy=merge_stateless(merge_selector=score_based_filtering(scoring_functions=[sf_miasm(shrink_strategy=shrink_bisimulation(greedy=false),max_states=50000,threshold_before_merge=1),total_order(atomic_ts_order=reverse_level,product_ts_order=new_to_old,atomic_before_product=false)])),label_reduction=exact(before_shrinking=true,before_merging=false),max_states=50000,threshold_before_merge=1))']),
+    # ('sccs-dfp-b50k', ['--search', 'astar(merge_and_shrink(shrink_strategy=shrink_bisimulation(greedy=false),merge_strategy=merge_sccs(order_of_sccs=topological,merge_selector=score_based_filtering(scoring_functions=[goal_relevance,dfp,total_order(atomic_ts_order=reverse_level,product_ts_order=new_to_old,atomic_before_product=false)])),label_reduction=exact(before_shrinking=true,before_merging=false),max_states=50000,threshold_before_merge=1))']),
+]
+CONFIGS = [
+    IssueConfig(
+        config_nick,
+        config,
+        build_options=[build],
+        driver_options=["--build", build])
+    for build in BUILDS
+    for config_nick, config in CONFIG_NICKS
+]
+
+SUITE = common_setup.DEFAULT_OPTIMAL_SUITE
+ENVIRONMENT = BaselSlurmEnvironment(
+    partition="infai_2",
+    email="silvan.sievers@unibas.ch",
+    export=["PATH", "DOWNWARD_BENCHMARKS"])
+
+if common_setup.is_test_run():
+    SUITE = IssueExperiment.DEFAULT_TEST_SUITE
+    ENVIRONMENT = LocalEnvironment(processes=4)
+
+exp = IssueExperiment(
+    revisions=REVISIONS,
+    configs=CONFIGS,
+    environment=ENVIRONMENT,
+)
+exp.add_suite(BENCHMARKS_DIR, SUITE)
+
+exp.add_parser(exp.EXITCODE_PARSER)
+exp.add_parser(exp.TRANSLATOR_PARSER)
+exp.add_parser(exp.SINGLE_SEARCH_PARSER)
+exp.add_parser(exp.PLANNER_PARSER)
+exp.add_parser('ms-parser.py')
+
+exp.add_step('build', exp.build)
+exp.add_step('start', exp.start_runs)
+exp.add_fetcher(name='fetch')
+
+# planner outcome attributes
+perfect_heuristic = Attribute('perfect_heuristic', absolute=True, min_wins=False)
+
+# m&s attributes
+ms_construction_time = Attribute('ms_construction_time', absolute=False, min_wins=True, functions=[geometric_mean])
+ms_atomic_construction_time = Attribute('ms_atomic_construction_time', absolute=False, min_wins=True, functions=[geometric_mean])
+ms_abstraction_constructed = Attribute('ms_abstraction_constructed', absolute=True, min_wins=False)
+ms_atomic_fts_constructed = Attribute('ms_atomic_fts_constructed', absolute=True, min_wins=False)
+ms_final_size = Attribute('ms_final_size', absolute=False, min_wins=True)
+ms_out_of_memory = Attribute('ms_out_of_memory', absolute=True, min_wins=True)
+ms_out_of_time = Attribute('ms_out_of_time', absolute=True, min_wins=True)
+search_out_of_memory = Attribute('search_out_of_memory', absolute=True, min_wins=True)
+search_out_of_time = Attribute('search_out_of_time', absolute=True, min_wins=True)
+
+extra_attributes = [
+    perfect_heuristic,
+
+    ms_construction_time,
+    ms_atomic_construction_time,
+    ms_abstraction_constructed,
+    ms_atomic_fts_constructed,
+    ms_final_size,
+    ms_out_of_memory,
+    ms_out_of_time,
+    search_out_of_memory,
+    search_out_of_time,
+]
+attributes = exp.DEFAULT_TABLE_ATTRIBUTES
+attributes.extend(extra_attributes)
+
+exp.add_comparison_table_step(attributes=attributes)
+
+exp.add_scatter_plot_step(attributes=[ms_atomic_construction_time])
+
+for algo_nick in ['dfp-b50k']: # 'rl-b50k', 'sbmiasm-b50k', 'sccs-dfp-b50k']:
+    algo = "issue851-v2-{}".format(algo_nick)
+    exp.add_report(
+        GeneralScatterPlotReport(
+            x_algo = algo,
+            y_algo = algo,
+            x_attribute='ms_atomic_construction_time',
+            y_attribute='total_time',
+            filter_algorithm=[algo],
+            attributes=['total_time'],
+            get_category=lambda run1, run2: run1["domain"],
+        ),
+        outfile='{}-total_time_vs_ms_atomic_construction_time.png'.format(algo),
+    )
+
+exp.run_steps()
diff -X lp-recognizer/fd-patch.ignore -ruN fast-downward-original/experiments/issue851/v3-debug.py fast-downward/experiments/issue851/v3-debug.py
--- fast-downward-original/experiments/issue851/v3-debug.py	1969-12-31 21:00:00.000000000 -0300
+++ fast-downward/experiments/issue851/v3-debug.py	2019-02-11 14:13:33.000000000 -0200
@@ -0,0 +1,97 @@
+#! /usr/bin/env python
+# -*- coding: utf-8 -*-
+
+import itertools
+import os
+
+from lab.environments import LocalEnvironment, BaselSlurmEnvironment
+from lab.reports import Attribute, geometric_mean
+
+from downward.reports.compare import ComparativeReport
+
+import common_setup
+from common_setup import IssueConfig, IssueExperiment
+from generalscatter import GeneralScatterPlotReport
+from relativescatter import RelativeScatterPlotReport
+
+DIR = os.path.dirname(os.path.abspath(__file__))
+SCRIPT_NAME = os.path.splitext(os.path.basename(__file__))[0]
+BENCHMARKS_DIR = os.environ["DOWNWARD_BENCHMARKS"]
+REVISIONS = ["issue851-v3"]
+BUILDS = ["debug32"]
+CONFIG_NICKS = [
+    ('dfp-b50k', ['--search', 'astar(merge_and_shrink(merge_strategy=merge_stateless(merge_selector=score_based_filtering(scoring_functions=[goal_relevance,dfp,total_order])),shrink_strategy=shrink_bisimulation(greedy=false),label_reduction=exact(before_shrinking=true,before_merging=false),max_states=50000,threshold_before_merge=1))']),
+    # ('rl-b50k', ['--search', 'astar(merge_and_shrink(merge_strategy=merge_precomputed(merge_tree=linear(variable_order=reverse_level)),shrink_strategy=shrink_bisimulation(greedy=false),label_reduction=exact(before_shrinking=true,before_merging=false),max_states=50000,threshold_before_merge=1))']),
+    # ('sbmiasm-b50k', ['--search', 'astar(merge_and_shrink(shrink_strategy=shrink_bisimulation(greedy=false),merge_strategy=merge_stateless(merge_selector=score_based_filtering(scoring_functions=[sf_miasm(shrink_strategy=shrink_bisimulation(greedy=false),max_states=50000,threshold_before_merge=1),total_order(atomic_ts_order=reverse_level,product_ts_order=new_to_old,atomic_before_product=false)])),label_reduction=exact(before_shrinking=true,before_merging=false),max_states=50000,threshold_before_merge=1))']),
+    # ('sccs-dfp-b50k', ['--search', 'astar(merge_and_shrink(shrink_strategy=shrink_bisimulation(greedy=false),merge_strategy=merge_sccs(order_of_sccs=topological,merge_selector=score_based_filtering(scoring_functions=[goal_relevance,dfp,total_order(atomic_ts_order=reverse_level,product_ts_order=new_to_old,atomic_before_product=false)])),label_reduction=exact(before_shrinking=true,before_merging=false),max_states=50000,threshold_before_merge=1))']),
+]
+CONFIGS = [
+    IssueConfig(
+        config_nick,
+        config,
+        build_options=[build],
+        driver_options=["--build", build])
+    for build in BUILDS
+    for config_nick, config in CONFIG_NICKS
+]
+
+SUITE = common_setup.DEFAULT_OPTIMAL_SUITE
+ENVIRONMENT = BaselSlurmEnvironment(
+    partition="infai_2",
+    email="silvan.sievers@unibas.ch",
+    export=["PATH", "DOWNWARD_BENCHMARKS"])
+
+if common_setup.is_test_run():
+    SUITE = IssueExperiment.DEFAULT_TEST_SUITE
+    ENVIRONMENT = LocalEnvironment(processes=4)
+
+exp = IssueExperiment(
+    revisions=REVISIONS,
+    configs=CONFIGS,
+    environment=ENVIRONMENT,
+)
+exp.add_suite(BENCHMARKS_DIR, SUITE)
+
+exp.add_parser(exp.EXITCODE_PARSER)
+exp.add_parser(exp.TRANSLATOR_PARSER)
+exp.add_parser(exp.SINGLE_SEARCH_PARSER)
+exp.add_parser(exp.PLANNER_PARSER)
+exp.add_parser('ms-parser.py')
+
+exp.add_step('build', exp.build)
+exp.add_step('start', exp.start_runs)
+exp.add_fetcher(name='fetch')
+
+# planner outcome attributes
+perfect_heuristic = Attribute('perfect_heuristic', absolute=True, min_wins=False)
+
+# m&s attributes
+ms_construction_time = Attribute('ms_construction_time', absolute=False, min_wins=True, functions=[geometric_mean])
+ms_atomic_construction_time = Attribute('ms_atomic_construction_time', absolute=False, min_wins=True, functions=[geometric_mean])
+ms_abstraction_constructed = Attribute('ms_abstraction_constructed', absolute=True, min_wins=False)
+ms_atomic_fts_constructed = Attribute('ms_atomic_fts_constructed', absolute=True, min_wins=False)
+ms_final_size = Attribute('ms_final_size', absolute=False, min_wins=True)
+ms_out_of_memory = Attribute('ms_out_of_memory', absolute=True, min_wins=True)
+ms_out_of_time = Attribute('ms_out_of_time', absolute=True, min_wins=True)
+search_out_of_memory = Attribute('search_out_of_memory', absolute=True, min_wins=True)
+search_out_of_time = Attribute('search_out_of_time', absolute=True, min_wins=True)
+
+extra_attributes = [
+    perfect_heuristic,
+
+    ms_construction_time,
+    ms_atomic_construction_time,
+    ms_abstraction_constructed,
+    ms_atomic_fts_constructed,
+    ms_final_size,
+    ms_out_of_memory,
+    ms_out_of_time,
+    search_out_of_memory,
+    search_out_of_time,
+]
+attributes = exp.DEFAULT_TABLE_ATTRIBUTES
+attributes.extend(extra_attributes)
+
+exp.add_absolute_report_step(attributes=attributes)
+
+exp.run_steps()
diff -X lp-recognizer/fd-patch.ignore -ruN fast-downward-original/experiments/issue851/v3.py fast-downward/experiments/issue851/v3.py
--- fast-downward-original/experiments/issue851/v3.py	1969-12-31 21:00:00.000000000 -0300
+++ fast-downward/experiments/issue851/v3.py	2019-02-11 14:13:33.000000000 -0200
@@ -0,0 +1,114 @@
+#! /usr/bin/env python
+# -*- coding: utf-8 -*-
+
+import itertools
+import os
+
+from lab.environments import LocalEnvironment, BaselSlurmEnvironment
+from lab.reports import Attribute, geometric_mean
+
+from downward.reports.compare import ComparativeReport
+
+import common_setup
+from common_setup import IssueConfig, IssueExperiment
+from generalscatter import GeneralScatterPlotReport
+from relativescatter import RelativeScatterPlotReport
+
+DIR = os.path.dirname(os.path.abspath(__file__))
+SCRIPT_NAME = os.path.splitext(os.path.basename(__file__))[0]
+BENCHMARKS_DIR = os.environ["DOWNWARD_BENCHMARKS"]
+REVISIONS = ["issue851-base-v2", "issue851-v2", "issue851-v3"]
+BUILDS = ["release32"]
+CONFIG_NICKS = [
+    ('dfp-b50k', ['--search', 'astar(merge_and_shrink(merge_strategy=merge_stateless(merge_selector=score_based_filtering(scoring_functions=[goal_relevance,dfp,total_order])),shrink_strategy=shrink_bisimulation(greedy=false),label_reduction=exact(before_shrinking=true,before_merging=false),max_states=50000,threshold_before_merge=1))']),
+    # ('rl-b50k', ['--search', 'astar(merge_and_shrink(merge_strategy=merge_precomputed(merge_tree=linear(variable_order=reverse_level)),shrink_strategy=shrink_bisimulation(greedy=false),label_reduction=exact(before_shrinking=true,before_merging=false),max_states=50000,threshold_before_merge=1))']),
+    # ('sbmiasm-b50k', ['--search', 'astar(merge_and_shrink(shrink_strategy=shrink_bisimulation(greedy=false),merge_strategy=merge_stateless(merge_selector=score_based_filtering(scoring_functions=[sf_miasm(shrink_strategy=shrink_bisimulation(greedy=false),max_states=50000,threshold_before_merge=1),total_order(atomic_ts_order=reverse_level,product_ts_order=new_to_old,atomic_before_product=false)])),label_reduction=exact(before_shrinking=true,before_merging=false),max_states=50000,threshold_before_merge=1))']),
+    # ('sccs-dfp-b50k', ['--search', 'astar(merge_and_shrink(shrink_strategy=shrink_bisimulation(greedy=false),merge_strategy=merge_sccs(order_of_sccs=topological,merge_selector=score_based_filtering(scoring_functions=[goal_relevance,dfp,total_order(atomic_ts_order=reverse_level,product_ts_order=new_to_old,atomic_before_product=false)])),label_reduction=exact(before_shrinking=true,before_merging=false),max_states=50000,threshold_before_merge=1))']),
+]
+CONFIGS = [
+    IssueConfig(
+        config_nick,
+        config,
+        build_options=[build],
+        driver_options=["--build", build])
+    for build in BUILDS
+    for config_nick, config in CONFIG_NICKS
+]
+
+SUITE = common_setup.DEFAULT_OPTIMAL_SUITE
+ENVIRONMENT = BaselSlurmEnvironment(
+    partition="infai_2",
+    email="silvan.sievers@unibas.ch",
+    export=["PATH", "DOWNWARD_BENCHMARKS"])
+
+if common_setup.is_test_run():
+    SUITE = IssueExperiment.DEFAULT_TEST_SUITE
+    ENVIRONMENT = LocalEnvironment(processes=4)
+
+exp = IssueExperiment(
+    revisions=REVISIONS,
+    configs=CONFIGS,
+    environment=ENVIRONMENT,
+)
+exp.add_suite(BENCHMARKS_DIR, SUITE)
+
+exp.add_parser(exp.EXITCODE_PARSER)
+exp.add_parser(exp.TRANSLATOR_PARSER)
+exp.add_parser(exp.SINGLE_SEARCH_PARSER)
+exp.add_parser(exp.PLANNER_PARSER)
+exp.add_parser('ms-parser.py')
+
+exp.add_step('build', exp.build)
+exp.add_step('start', exp.start_runs)
+exp.add_fetcher(name='fetch')
+
+# planner outcome attributes
+perfect_heuristic = Attribute('perfect_heuristic', absolute=True, min_wins=False)
+
+# m&s attributes
+ms_construction_time = Attribute('ms_construction_time', absolute=False, min_wins=True, functions=[geometric_mean])
+ms_atomic_construction_time = Attribute('ms_atomic_construction_time', absolute=False, min_wins=True, functions=[geometric_mean])
+ms_abstraction_constructed = Attribute('ms_abstraction_constructed', absolute=True, min_wins=False)
+ms_atomic_fts_constructed = Attribute('ms_atomic_fts_constructed', absolute=True, min_wins=False)
+ms_final_size = Attribute('ms_final_size', absolute=False, min_wins=True)
+ms_out_of_memory = Attribute('ms_out_of_memory', absolute=True, min_wins=True)
+ms_out_of_time = Attribute('ms_out_of_time', absolute=True, min_wins=True)
+search_out_of_memory = Attribute('search_out_of_memory', absolute=True, min_wins=True)
+search_out_of_time = Attribute('search_out_of_time', absolute=True, min_wins=True)
+
+extra_attributes = [
+    perfect_heuristic,
+
+    ms_construction_time,
+    ms_atomic_construction_time,
+    ms_abstraction_constructed,
+    ms_atomic_fts_constructed,
+    ms_final_size,
+    ms_out_of_memory,
+    ms_out_of_time,
+    search_out_of_memory,
+    search_out_of_time,
+]
+attributes = exp.DEFAULT_TABLE_ATTRIBUTES
+attributes.extend(extra_attributes)
+
+exp.add_comparison_table_step(attributes=attributes)
+
+exp.add_scatter_plot_step(attributes=[ms_atomic_construction_time])
+
+for algo_nick in ['dfp-b50k']: # 'rl-b50k', 'sbmiasm-b50k', 'sccs-dfp-b50k']:
+    algo = "issue851-v2-{}".format(algo_nick)
+    exp.add_report(
+        GeneralScatterPlotReport(
+            x_algo = algo,
+            y_algo = algo,
+            x_attribute='ms_atomic_construction_time',
+            y_attribute='total_time',
+            filter_algorithm=[algo],
+            attributes=['total_time'],
+            get_category=lambda run1, run2: run1["domain"],
+        ),
+        outfile='{}-total_time_vs_ms_atomic_construction_time.png'.format(algo),
+    )
+
+exp.run_steps()
diff -X lp-recognizer/fd-patch.ignore -ruN fast-downward-original/experiments/issue851/v4.py fast-downward/experiments/issue851/v4.py
--- fast-downward-original/experiments/issue851/v4.py	1969-12-31 21:00:00.000000000 -0300
+++ fast-downward/experiments/issue851/v4.py	2019-02-11 14:13:33.000000000 -0200
@@ -0,0 +1,114 @@
+#! /usr/bin/env python
+# -*- coding: utf-8 -*-
+
+import itertools
+import os
+
+from lab.environments import LocalEnvironment, BaselSlurmEnvironment
+from lab.reports import Attribute, geometric_mean
+
+from downward.reports.compare import ComparativeReport
+
+import common_setup
+from common_setup import IssueConfig, IssueExperiment
+from generalscatter import GeneralScatterPlotReport
+from relativescatter import RelativeScatterPlotReport
+
+DIR = os.path.dirname(os.path.abspath(__file__))
+SCRIPT_NAME = os.path.splitext(os.path.basename(__file__))[0]
+BENCHMARKS_DIR = os.environ["DOWNWARD_BENCHMARKS"]
+REVISIONS = ["issue851-base-v2", "issue851-v3", "issue851-v4"]
+BUILDS = ["release32"]
+CONFIG_NICKS = [
+    ('dfp-b50k', ['--search', 'astar(merge_and_shrink(merge_strategy=merge_stateless(merge_selector=score_based_filtering(scoring_functions=[goal_relevance,dfp,total_order])),shrink_strategy=shrink_bisimulation(greedy=false),label_reduction=exact(before_shrinking=true,before_merging=false),max_states=50000,threshold_before_merge=1))']),
+    # ('rl-b50k', ['--search', 'astar(merge_and_shrink(merge_strategy=merge_precomputed(merge_tree=linear(variable_order=reverse_level)),shrink_strategy=shrink_bisimulation(greedy=false),label_reduction=exact(before_shrinking=true,before_merging=false),max_states=50000,threshold_before_merge=1))']),
+    # ('sbmiasm-b50k', ['--search', 'astar(merge_and_shrink(shrink_strategy=shrink_bisimulation(greedy=false),merge_strategy=merge_stateless(merge_selector=score_based_filtering(scoring_functions=[sf_miasm(shrink_strategy=shrink_bisimulation(greedy=false),max_states=50000,threshold_before_merge=1),total_order(atomic_ts_order=reverse_level,product_ts_order=new_to_old,atomic_before_product=false)])),label_reduction=exact(before_shrinking=true,before_merging=false),max_states=50000,threshold_before_merge=1))']),
+    # ('sccs-dfp-b50k', ['--search', 'astar(merge_and_shrink(shrink_strategy=shrink_bisimulation(greedy=false),merge_strategy=merge_sccs(order_of_sccs=topological,merge_selector=score_based_filtering(scoring_functions=[goal_relevance,dfp,total_order(atomic_ts_order=reverse_level,product_ts_order=new_to_old,atomic_before_product=false)])),label_reduction=exact(before_shrinking=true,before_merging=false),max_states=50000,threshold_before_merge=1))']),
+]
+CONFIGS = [
+    IssueConfig(
+        config_nick,
+        config,
+        build_options=[build],
+        driver_options=["--build", build])
+    for build in BUILDS
+    for config_nick, config in CONFIG_NICKS
+]
+
+SUITE = common_setup.DEFAULT_OPTIMAL_SUITE
+ENVIRONMENT = BaselSlurmEnvironment(
+    partition="infai_2",
+    email="silvan.sievers@unibas.ch",
+    export=["PATH", "DOWNWARD_BENCHMARKS"])
+
+if common_setup.is_test_run():
+    SUITE = IssueExperiment.DEFAULT_TEST_SUITE
+    ENVIRONMENT = LocalEnvironment(processes=4)
+
+exp = IssueExperiment(
+    revisions=REVISIONS,
+    configs=CONFIGS,
+    environment=ENVIRONMENT,
+)
+exp.add_suite(BENCHMARKS_DIR, SUITE)
+
+exp.add_parser(exp.EXITCODE_PARSER)
+exp.add_parser(exp.TRANSLATOR_PARSER)
+exp.add_parser(exp.SINGLE_SEARCH_PARSER)
+exp.add_parser(exp.PLANNER_PARSER)
+exp.add_parser('ms-parser.py')
+
+exp.add_step('build', exp.build)
+exp.add_step('start', exp.start_runs)
+exp.add_fetcher(name='fetch')
+
+# planner outcome attributes
+perfect_heuristic = Attribute('perfect_heuristic', absolute=True, min_wins=False)
+
+# m&s attributes
+ms_construction_time = Attribute('ms_construction_time', absolute=False, min_wins=True, functions=[geometric_mean])
+ms_atomic_construction_time = Attribute('ms_atomic_construction_time', absolute=False, min_wins=True, functions=[geometric_mean])
+ms_abstraction_constructed = Attribute('ms_abstraction_constructed', absolute=True, min_wins=False)
+ms_atomic_fts_constructed = Attribute('ms_atomic_fts_constructed', absolute=True, min_wins=False)
+ms_final_size = Attribute('ms_final_size', absolute=False, min_wins=True)
+ms_out_of_memory = Attribute('ms_out_of_memory', absolute=True, min_wins=True)
+ms_out_of_time = Attribute('ms_out_of_time', absolute=True, min_wins=True)
+search_out_of_memory = Attribute('search_out_of_memory', absolute=True, min_wins=True)
+search_out_of_time = Attribute('search_out_of_time', absolute=True, min_wins=True)
+
+extra_attributes = [
+    perfect_heuristic,
+
+    ms_construction_time,
+    ms_atomic_construction_time,
+    ms_abstraction_constructed,
+    ms_atomic_fts_constructed,
+    ms_final_size,
+    ms_out_of_memory,
+    ms_out_of_time,
+    search_out_of_memory,
+    search_out_of_time,
+]
+attributes = exp.DEFAULT_TABLE_ATTRIBUTES
+attributes.extend(extra_attributes)
+
+exp.add_comparison_table_step(attributes=attributes)
+
+exp.add_scatter_plot_step(attributes=[ms_atomic_construction_time])
+
+for algo_nick in ['dfp-b50k']: # 'rl-b50k', 'sbmiasm-b50k', 'sccs-dfp-b50k']:
+    algo = "issue851-v2-{}".format(algo_nick)
+    exp.add_report(
+        GeneralScatterPlotReport(
+            x_algo = algo,
+            y_algo = algo,
+            x_attribute='ms_atomic_construction_time',
+            y_attribute='total_time',
+            filter_algorithm=[algo],
+            attributes=['total_time'],
+            get_category=lambda run1, run2: run1["domain"],
+        ),
+        outfile='{}-total_time_vs_ms_atomic_construction_time.png'.format(algo),
+    )
+
+exp.run_steps()
diff -X lp-recognizer/fd-patch.ignore -ruN fast-downward-original/experiments/issue880/v1.py fast-downward/experiments/issue880/v1.py
--- fast-downward-original/experiments/issue880/v1.py	2019-02-11 14:16:41.000000000 -0200
+++ fast-downward/experiments/issue880/v1.py	2019-02-11 14:13:33.000000000 -0200
@@ -69,7 +69,7 @@
 exp.add_comparison_table_step(attributes=attributes)
 
 if len(REVISIONS) == 2:
-    for attribute in ["init_time", "expansions_until_last_jump"]:
+    for attribute in ["init_time", "expansions_until_last_jump", "total_time_for_splitting_states", "total_time_for_finding_traces"]:
         for config in CONFIGS:
             exp.add_report(
                 RelativeScatterPlotReport(
diff -X lp-recognizer/fd-patch.ignore -ruN fast-downward-original/experiments/issue880/v2-900s.py fast-downward/experiments/issue880/v2-900s.py
--- fast-downward-original/experiments/issue880/v2-900s.py	1969-12-31 21:00:00.000000000 -0300
+++ fast-downward/experiments/issue880/v2-900s.py	2019-02-11 14:13:33.000000000 -0200
@@ -0,0 +1,79 @@
+#! /usr/bin/env python
+# -*- coding: utf-8 -*-
+
+import os
+
+from lab.environments import LocalEnvironment, BaselSlurmEnvironment
+
+import common_setup
+from common_setup import IssueConfig, IssueExperiment
+from relativescatter import RelativeScatterPlotReport
+
+DIR = os.path.dirname(os.path.abspath(__file__))
+BENCHMARKS_DIR = os.environ["DOWNWARD_BENCHMARKS"]
+BUILD = "release64"
+REVISIONS = ["issue880-base", "issue880-v2"]
+DRIVER_OPTIONS = ["--build", BUILD]
+CONFIGS = [
+    IssueConfig(
+        nick,
+        config,
+        build_options=[BUILD],
+        driver_options=DRIVER_OPTIONS)
+    for nick, config in [
+        ("cegar-original-900s", ["--search", "astar(cegar(subtasks=[original()], max_transitions=infinity, max_time=900))".format(**locals())]),
+        ]
+]
+SUITE = common_setup.DEFAULT_OPTIMAL_SUITE
+ENVIRONMENT = BaselSlurmEnvironment(
+    partition="infai_1",
+    email="jendrik.seipp@unibas.ch",
+    export=["PATH", "DOWNWARD_BENCHMARKS"])
+
+if common_setup.is_test_run():
+    SUITE = [
+        "depot:p01.pddl",
+        "gripper:prob01.pddl"]
+    ENVIRONMENT = LocalEnvironment(processes=1)
+
+exp = IssueExperiment(
+    revisions=REVISIONS,
+    configs=CONFIGS,
+    environment=ENVIRONMENT,
+)
+exp.add_suite(BENCHMARKS_DIR, SUITE)
+
+exp.add_parser(exp.EXITCODE_PARSER)
+#exp.add_parser(exp.TRANSLATOR_PARSER)
+exp.add_parser(exp.SINGLE_SEARCH_PARSER)
+exp.add_parser(exp.PLANNER_PARSER)
+exp.add_parser(os.path.join(DIR, "parser.py"))
+
+exp.add_step('build', exp.build)
+exp.add_step('start', exp.start_runs)
+exp.add_fetcher(name='fetch')
+
+REFINEMENT_ATTRIBUTES = [
+    "time_for_finding_traces",
+    "time_for_finding_flaws",
+    "time_for_splitting_states",
+]
+attributes = (
+    IssueExperiment.DEFAULT_TABLE_ATTRIBUTES +
+    ["search_start_memory", "init_time", "time_analysis"] +
+    REFINEMENT_ATTRIBUTES +
+    ["total_" + attr for attr in REFINEMENT_ATTRIBUTES])
+#exp.add_absolute_report_step(attributes=attributes)
+exp.add_comparison_table_step(attributes=attributes)
+
+if len(REVISIONS) == 2:
+    for attribute in ["init_time", "expansions_until_last_jump"]:
+        for config in CONFIGS:
+            exp.add_report(
+                RelativeScatterPlotReport(
+                    attributes=[attribute],
+                    filter_algorithm=["{}-{}".format(rev, config.nick) for rev in REVISIONS],
+                    get_category=lambda run1, run2: run1.get("domain")),
+                outfile="{}-{}-{}-{}-{}.png".format(exp.name, attribute, config.nick, *REVISIONS))
+
+exp.run_steps()
diff -X lp-recognizer/fd-patch.ignore -ruN fast-downward-original/experiments/issue880/v2-max-transitions.py fast-downward/experiments/issue880/v2-max-transitions.py
--- fast-downward-original/experiments/issue880/v2-max-transitions.py	1969-12-31 21:00:00.000000000 -0300
+++ fast-downward/experiments/issue880/v2-max-transitions.py	2019-02-11 14:13:33.000000000 -0200
@@ -0,0 +1,80 @@
+#! /usr/bin/env python
+# -*- coding: utf-8 -*-
+
+import os
+
+from lab.environments import LocalEnvironment, BaselSlurmEnvironment
+
+import common_setup
+from common_setup import IssueConfig, IssueExperiment
+from relativescatter import RelativeScatterPlotReport
+
+DIR = os.path.dirname(os.path.abspath(__file__))
+BENCHMARKS_DIR = os.environ["DOWNWARD_BENCHMARKS"]
+BUILD = "release64"
+REVISIONS = ["issue880-base", "issue880-v2"]
+DRIVER_OPTIONS = ["--build", BUILD]
+CONFIGS = [
+    IssueConfig(
+        "{nick}-{million_transitions}M".format(**locals()),
+        config,
+        build_options=[BUILD],
+        driver_options=DRIVER_OPTIONS)
+    for million_transitions in [1, 2, 5, 10]
+    for nick, config in [
+        ("cegar-original", ["--search", "astar(cegar(subtasks=[original()], max_transitions={max_transitions}))".format(max_transitions=million_transitions * 10**6)]),
+        ("cegar-landmarks-goals", ["--search", "astar(cegar(max_transitions={max_transitions}))".format(max_transitions=million_transitions * 10**6)]),
+        ]
+]
+SUITE = common_setup.DEFAULT_OPTIMAL_SUITE
+ENVIRONMENT = BaselSlurmEnvironment(
+    partition="infai_2",
+    email="jendrik.seipp@unibas.ch",
+    export=["PATH", "DOWNWARD_BENCHMARKS"])
+
+if common_setup.is_test_run():
+    SUITE = [
+        "depot:p01.pddl",
+        "gripper:prob01.pddl"]
+    ENVIRONMENT = LocalEnvironment(processes=1)
+
+exp = IssueExperiment(
+    revisions=REVISIONS,
+    configs=CONFIGS,
+    environment=ENVIRONMENT,
+)
+exp.add_suite(BENCHMARKS_DIR, SUITE)
+
+exp.add_parser(exp.EXITCODE_PARSER)
+#exp.add_parser(exp.TRANSLATOR_PARSER)
+exp.add_parser(exp.SINGLE_SEARCH_PARSER)
+exp.add_parser(exp.PLANNER_PARSER)
+exp.add_parser(os.path.join(DIR, "parser.py"))
+
+exp.add_step('build', exp.build)
+exp.add_step('start', exp.start_runs)
+exp.add_fetcher(name='fetch')
+
+REFINEMENT_ATTRIBUTES = [
+    "time_for_finding_traces",
+    "time_for_finding_flaws",
+    "time_for_splitting_states",
+]
+attributes = (
+    IssueExperiment.DEFAULT_TABLE_ATTRIBUTES +
+    ["search_start_memory", "init_time", "time_analysis"] +
+    ["total_" + attr for attr in REFINEMENT_ATTRIBUTES])
+#exp.add_absolute_report_step(attributes=attributes)
+exp.add_comparison_table_step(attributes=attributes)
+
+if len(REVISIONS) == 2:
+    for attribute in ["init_time", "expansions_until_last_jump"]:
+        for config in CONFIGS:
+            exp.add_report(
+                RelativeScatterPlotReport(
+                    attributes=[attribute],
+                    filter_algorithm=["{}-{}".format(rev, config.nick) for rev in REVISIONS],
+                    get_category=lambda run1, run2: run1.get("domain")),
+                outfile="{}-{}-{}-{}-{}.png".format(exp.name, attribute, config.nick, *REVISIONS))
+
+exp.run_steps()
diff -X lp-recognizer/fd-patch.ignore -ruN fast-downward-original/experiments/issue880/v2.py fast-downward/experiments/issue880/v2.py
--- fast-downward-original/experiments/issue880/v2.py	1969-12-31 21:00:00.000000000 -0300
+++ fast-downward/experiments/issue880/v2.py	2019-02-11 14:13:33.000000000 -0200
@@ -0,0 +1,81 @@
+#! /usr/bin/env python
+# -*- coding: utf-8 -*-
+
+import os
+
+from lab.environments import LocalEnvironment, BaselSlurmEnvironment
+
+import common_setup
+from common_setup import IssueConfig, IssueExperiment
+from relativescatter import RelativeScatterPlotReport
+
+DIR = os.path.dirname(os.path.abspath(__file__))
+BENCHMARKS_DIR = os.environ["DOWNWARD_BENCHMARKS"]
+BUILD = "release64"
+REVISIONS = ["issue880-v1", "issue880-v2"]
+DRIVER_OPTIONS = ["--build", BUILD]
+CONFIGS = [
+    IssueConfig(
+        nick + "-" + max_transitions_nick,
+        config,
+        build_options=[BUILD],
+        driver_options=DRIVER_OPTIONS)
+    for max_transitions_nick, max_transitions in [("1M", 1000000)]
+    for nick, config in [
+        ("cegar-original", ["--search", "astar(cegar(subtasks=[original()], max_transitions={max_transitions}))".format(**locals())]),
+        #("cegar-landmarks-goals", ["--search", "astar(cegar(max_transitions={max_transitions}))".format(**locals())]),
+        ]
+]
+SUITE = common_setup.DEFAULT_OPTIMAL_SUITE
+ENVIRONMENT = BaselSlurmEnvironment(
+    partition="infai_2",
+    email="jendrik.seipp@unibas.ch",
+    export=["PATH", "DOWNWARD_BENCHMARKS"])
+
+if common_setup.is_test_run():
+    SUITE = [
+        "depot:p01.pddl",
+        "gripper:prob01.pddl"]
+    ENVIRONMENT = LocalEnvironment(processes=1)
+
+exp = IssueExperiment(
+    revisions=REVISIONS,
+    configs=CONFIGS,
+    environment=ENVIRONMENT,
+)
+exp.add_suite(BENCHMARKS_DIR, SUITE)
+
+exp.add_parser(exp.EXITCODE_PARSER)
+#exp.add_parser(exp.TRANSLATOR_PARSER)
+exp.add_parser(exp.SINGLE_SEARCH_PARSER)
+exp.add_parser(exp.PLANNER_PARSER)
+exp.add_parser(os.path.join(DIR, "parser.py"))
+
+exp.add_step('build', exp.build)
+exp.add_step('start', exp.start_runs)
+exp.add_fetcher(name='fetch')
+
+REFINEMENT_ATTRIBUTES = [
+    "time_for_finding_traces",
+    "time_for_finding_flaws",
+    "time_for_splitting_states",
+]
+attributes = (
+    IssueExperiment.DEFAULT_TABLE_ATTRIBUTES +
+    ["search_start_memory", "init_time", "time_analysis"] +
+    REFINEMENT_ATTRIBUTES +
+    ["total_" + attr for attr in REFINEMENT_ATTRIBUTES])
+#exp.add_absolute_report_step(attributes=attributes)
+exp.add_comparison_table_step(attributes=attributes)
+
+if len(REVISIONS) == 2:
+    for attribute in ["init_time", "expansions_until_last_jump"]:
+        for config in CONFIGS:
+            exp.add_report(
+                RelativeScatterPlotReport(
+                    attributes=[attribute],
+                    filter_algorithm=["{}-{}".format(rev, config.nick) for rev in REVISIONS],
+                    get_category=lambda run1, run2: run1.get("domain")),
+                outfile="{}-{}-{}-{}-{}.png".format(exp.name, attribute, config.nick, *REVISIONS))
+
+exp.run_steps()
diff -X lp-recognizer/fd-patch.ignore -ruN fast-downward-original/fast-downward fast-downward/fast-downward
--- fast-downward-original/fast-downward	1969-12-31 21:00:00.000000000 -0300
+++ fast-downward/fast-downward	2019-02-11 14:15:21.000000000 -0200
@@ -0,0 +1,13 @@
+#!/usr/bin/env bash
+DIR=`dirname $0`
+DOMAIN=$1
+shift
+PROBLEM=$1
+shift
+# ${DIR}/fast-downward.py $DOMAIN $PROBLEM --search "astar(lmcut())" "$@"
+# ${DIR}/fast-downward.py $DOMAIN $PROBLEM --search "astar(seq())" "$@"
+# ${DIR}/fast-downward.py --build=release64 $DOMAIN $PROBLEM --search "astar(operatorcounting([lmcut_constraints()], lpsolver=CPLEX, transform=no_transform(), cache_estimates=true))" "$@"
+# ${DIR}/fast-downward.py --build=release64 $DOMAIN $PROBLEM --search "astar(operatorcounting([lmcut_constraints(), pho_constraints(), state_equation_constraints()]))" "$@"
+${DIR}/fast-downward.py --build=release64 $DOMAIN $PROBLEM --search "astar(operatorcounting([lmcut_constraints(), pho_constraints(), state_equation_constraints()]))" "$@"
+
+# ${DIR}/fast-downward.py $DOMAIN $PROBLEM --alias seq-opt-lmcut "$@"
\ No newline at end of file
diff -X lp-recognizer/fd-patch.ignore -ruN fast-downward-original/fd-constraints fast-downward/fd-constraints
--- fast-downward-original/fd-constraints	1969-12-31 21:00:00.000000000 -0300
+++ fast-downward/fd-constraints	2019-02-11 14:15:21.000000000 -0200
@@ -0,0 +1,14 @@
+#!/usr/bin/env bash
+DIR=`dirname $0`
+DOMAIN=$1
+shift
+PROBLEM=$1
+shift
+# ${DIR}/fast-downward.py $DOMAIN $PROBLEM --search "astar(lmcut())" "$@"
+# ${DIR}/fast-downward.py $DOMAIN $PROBLEM --search "astar(seq())" "$@"
+# ${DIR}/fast-downward.py --build=release64 $DOMAIN $PROBLEM --search "astar(operatorcounting([lmcut_constraints()], lpsolver=CPLEX, transform=no_transform(), cache_estimates=true))" "$@"
+# ${DIR}/fast-downward.py --build=release64 $DOMAIN $PROBLEM --search "astar(operatorcounting([lmcut_constraints(), pho_constraints(), state_equation_constraints()]))" "$@"
+
+# ${DIR}/builds/debug64/bin/downward --search "astar(ocsingleshot([lmcut_constraints()], lpsolver=CPLEX, transform=no_transform(), cache_estimates=true))" --internal-plan-file ${DIR}/sas_plan < output.sas
+
+${DIR}/fast-downward.py --build=release64 $DOMAIN $PROBLEM --search "astar(ocsingleshot([lmcut_constraints(), pho_constraints(), state_equation_constraints()],enforce_observations=false))" "$@"
\ No newline at end of file
diff -X lp-recognizer/fd-patch.ignore -ruN fast-downward-original/install-osi-linux.sh fast-downward/install-osi-linux.sh
--- fast-downward-original/install-osi-linux.sh	1969-12-31 21:00:00.000000000 -0300
+++ fast-downward/install-osi-linux.sh	2019-02-11 14:15:21.000000000 -0200
@@ -0,0 +1,21 @@
+#!/usr/bin/env bash
+export DOWNWARD_CPLEX_ROOT64=/opt/ibm/ILOG/CPLEX_Studio_Community128/cplex
+# You should probably change the line below to match where you want COIN64
+export DOWNWARD_COIN_ROOT64=~/workspace-planning/coin64 
+pushd ..
+wget -c http://www.coin-or.org/download/source/Osi/Osi-0.107.9.tgz
+tar xvzf Osi-0.107.9.tgz
+cd Osi-0.107.9
+./configure CC="gcc"  CFLAGS="-m64 -pthread -Wno-long-long" \
+            CXX="g++" CXXFLAGS="-m64 -pthread -Wno-long-long" \
+            LDFLAGS="-L$DOWNWARD_CPLEX_ROOT64/lib/x86-64_linux/static_pic" \
+            --without-lapack --enable-static=yes \
+            --prefix="$DOWNWARD_COIN_ROOT64" \
+            --disable-zlib --disable-bzlib \
+            --with-cplex-incdir=$DOWNWARD_CPLEX_ROOT64/include/ilcplex --with-cplex-lib="-lcplex -lm"
+make
+make install
+cd ..
+# rm -rf Osi-0.107.9
+# rm Osi-0.107.9.tgz
+popd
\ No newline at end of file
diff -X lp-recognizer/fd-patch.ignore -ruN fast-downward-original/install-osi-mac.sh fast-downward/install-osi-mac.sh
--- fast-downward-original/install-osi-mac.sh	1969-12-31 21:00:00.000000000 -0300
+++ fast-downward/install-osi-mac.sh	2019-02-11 14:15:21.000000000 -0200
@@ -0,0 +1,23 @@
+#!/usr/bin/env bash
+pushd ..
+export DOWNWARD_CPLEX_ROOT64=/Applications/CPLEX_Studio_Community128/cplex
+# You should probably change the line below to match where you want COIN64
+export DOWNWARD_COIN_ROOT64=~/Documents/workspace-planning/coin64 
+wget -c http://www.coin-or.org/download/source/Osi/Osi-0.107.9.tgz
+tar xvzf Osi-0.107.9.tgz
+cd Osi-0.107.9
+
+./configure CC="gcc"  CFLAGS="-m64 -arch x86_64 -pthread -Wno-long-long" \
+            CXX="g++" CXXFLAGS="-m64 -arch x86_64 -pthread -Wno-long-long" \
+            LDFLAGS="-L$DOWNWARD_CPLEX_ROOT64/lib/x86-64_osx/static_pic -arch x86_64 -v" \
+            --without-lapack --disable-shared --enable-static=yes \
+            --prefix="$DOWNWARD_COIN_ROOT64" \
+            --disable-zlib --disable-bzlib \
+            --with-cplex-incdir=$DOWNWARD_CPLEX_ROOT64/include/ilcplex --with-cplex-lib="-lcplex -lm -ldl"
+
+make -j8
+make install
+cd ..
+# rm -rf Osi-0.107.9
+# rm Osi-0.107.9.tgz
+popd
\ No newline at end of file
diff -X lp-recognizer/fd-patch.ignore -ruN fast-downward-original/new_constraint.txt fast-downward/new_constraint.txt
--- fast-downward-original/new_constraint.txt	1969-12-31 21:00:00.000000000 -0300
+++ fast-downward/new_constraint.txt	2019-02-11 14:15:21.000000000 -0200
@@ -0,0 +1,3 @@
+2 1e+20
+0 4
+1 1
diff -X lp-recognizer/fd-patch.ignore -ruN fast-downward-original/src/cmake_modules/FastDownwardMacros.cmake fast-downward/src/cmake_modules/FastDownwardMacros.cmake
--- fast-downward-original/src/cmake_modules/FastDownwardMacros.cmake	2019-02-11 14:16:41.000000000 -0200
+++ fast-downward/src/cmake_modules/FastDownwardMacros.cmake	2019-02-11 14:15:21.000000000 -0200
@@ -18,7 +18,7 @@
 
         ## Configuration-specific flags
         set(CMAKE_CXX_FLAGS_RELEASE "-O3 -DNDEBUG -fomit-frame-pointer")
-        set(CMAKE_CXX_FLAGS_DEBUG "-O3 -D_GLIBCXX_DEBUG")
+        set(CMAKE_CXX_FLAGS_DEBUG "-O0 -D_GLIBCXX_DEBUG")
         set(CMAKE_CXX_FLAGS_PROFILE "-O3 -pg")
     elseif(MSVC)
         # We force linking to be static on Windows because this makes compiling OSI simpler
diff -X lp-recognizer/fd-patch.ignore -ruN fast-downward-original/src/search/DownwardFiles.cmake fast-downward/src/search/DownwardFiles.cmake
--- fast-downward-original/src/search/DownwardFiles.cmake	2019-02-11 14:16:41.000000000 -0200
+++ fast-downward/src/search/DownwardFiles.cmake	2019-02-11 14:15:21.000000000 -0200
@@ -698,6 +698,18 @@
 )
 
 fast_downward_plugin(
+    NAME OC_SINGLESHOT
+    HELP "Plugin containing the code for operator counting single shot"
+    SOURCES
+        operator_counting/constraint_generator
+        operator_counting/lm_cut_constraints
+        operator_counting/oc_single_shot_heuristic
+        operator_counting/pho_constraints
+        operator_counting/state_equation_constraints
+    DEPENDS LP_SOLVER LANDMARK_CUT_HEURISTIC PDBS TASK_PROPERTIES
+)
+
+fast_downward_plugin(
     NAME PDBS
     HELP "Plugin containing the code for PDBs"
     SOURCES
diff -X lp-recognizer/fd-patch.ignore -ruN fast-downward-original/src/search/cegar/abstract_search.cc fast-downward/src/search/cegar/abstract_search.cc
--- fast-downward-original/src/search/cegar/abstract_search.cc	2019-02-11 14:16:41.000000000 -0200
+++ fast-downward/src/search/cegar/abstract_search.cc	2019-02-11 14:13:33.000000000 -0200
@@ -46,15 +46,15 @@
     return solution;
 }
 
-void AbstractSearch::update_goal_distances(const Solution &solution, int init_id) {
-    int goal_distance = 0;
-    for (auto it = solution.rbegin(); it != solution.rend(); ++it) {
-        const Transition &transition = *it;
-        int current_state = transition.target_id;
-        set_h_value(current_state, goal_distance);
-        goal_distance += operator_costs[transition.op_id];
+void AbstractSearch::update_goal_distances(const Solution &solution) {
+    int solution_cost = 0;
+    for (const Transition &transition : solution) {
+        solution_cost += operator_costs[transition.op_id];
+    }
+    for (auto &info : search_info) {
+        int new_h = max(info.get_h_value(), solution_cost - info.get_g_value());
+        info.increase_h_value_to(new_h);
     }
-    set_h_value(init_id, goal_distance);
 }
 
 unique_ptr<Solution> AbstractSearch::find_solution(
@@ -64,12 +64,13 @@
     reset(transitions.size());
     search_info[init_id].decrease_g_value_to(0);
     open_queue.push(search_info[init_id].get_h_value(), init_id);
+
     int goal_id = astar_search(transitions, true, &goal_ids);
     open_queue.clear();
     bool has_found_solution = (goal_id != UNDEFINED);
     if (has_found_solution) {
         unique_ptr<Solution> solution = extract_solution(init_id, goal_id);
-        update_goal_distances(*solution, init_id);
+        update_goal_distances(*solution);
         return solution;
     } else {
         search_info[init_id].increase_h_value_to(INF);
@@ -145,7 +146,7 @@
 
 void AbstractSearch::set_h_value(int state_id, int h) {
     assert(utils::in_bounds(state_id, search_info));
-    return search_info[state_id].increase_h_value_to(h);
+    search_info[state_id].increase_h_value_to(h);
 }
 
 void AbstractSearch::copy_h_value_to_children(int v, int v1, int v2) {
diff -X lp-recognizer/fd-patch.ignore -ruN fast-downward-original/src/search/cegar/abstract_search.h fast-downward/src/search/cegar/abstract_search.h
--- fast-downward-original/src/search/cegar/abstract_search.h	2019-02-11 14:16:41.000000000 -0200
+++ fast-downward/src/search/cegar/abstract_search.h	2019-02-11 14:13:33.000000000 -0200
@@ -75,7 +75,7 @@
     std::vector<int> get_g_values() const;
 
     std::unique_ptr<Solution> extract_solution(int init_id, int goal_id) const;
-    void update_goal_distances(const Solution &solution, int init_id);
+    void update_goal_distances(const Solution &solution);
 
     int astar_search(
         const std::vector<Transitions> &transitions,
diff -X lp-recognizer/fd-patch.ignore -ruN fast-downward-original/src/search/lp/lp_solver.cc fast-downward/src/search/lp/lp_solver.cc
--- fast-downward-original/src/search/lp/lp_solver.cc	2019-02-11 14:16:39.000000000 -0200
+++ fast-downward/src/search/lp/lp_solver.cc	2019-02-11 14:15:21.000000000 -0200
@@ -3,7 +3,7 @@
 #include "lp_internals.h"
 
 #include "../option_parser.h"
-
+#include "../utils/logging.h"
 #include "../utils/system.h"
 
 #ifdef USE_LP
@@ -67,10 +67,11 @@
 }
 
 LPVariable::LPVariable(double lower_bound, double upper_bound,
-                       double objective_coefficient)
+                       double objective_coefficient, bool is_integer)
     : lower_bound(lower_bound),
       upper_bound(upper_bound),
-      objective_coefficient(objective_coefficient) {
+      objective_coefficient(objective_coefficient),
+      is_integer(is_integer){
 }
 
 LPSolver::~LPSolver() {
@@ -105,6 +106,10 @@
     is_initialized = false;
     num_permanent_constraints = constraints.size();
 
+    ///////////////////////////////////////////////////////////////////
+    all_constraints = constraints;
+    ///////////////////////////////////////////////////////////////////
+
     for (const LPVariable &var : variables) {
         col_lb.push_back(var.lower_bound);
         col_ub.push_back(var.upper_bound);
@@ -156,6 +161,14 @@
                                objective.data(),
                                row_lb.data(),
                                row_ub.data());
+        int num_vars = variables.size();
+        for (int i = 0; i < num_vars; ++i) {
+            if (variables[i].is_integer) {
+                lp_solver->setInteger(i);
+            }
+        }
+        //cout << "Integer tolerance: " << lp_solver->getIntegerTolerance() << endl;
+        //cout << "Number of integer variables: " << lp_solver->getNumIntegers() << endl;
     } catch (CoinError &error) {
         handle_coin_error(error);
     }
@@ -176,7 +189,6 @@
                                constraint.get_coefficients().data(),
                                false));
         }
-
         try {
             lp_solver->addRows(num_rows,
                                rows.data(), row_lb.data(), row_ub.data());
@@ -192,6 +204,36 @@
     }
 }
 
+///////////////////////////////////////////////////////////////////
+void LPSolver::add_new_constraints(const vector<LPConstraint> &constraints) {
+    if (!constraints.empty()) {
+        clear_temporary_data();
+        int num_rows = constraints.size();
+        for (const LPConstraint &constraint : constraints) {
+            row_lb.push_back(constraint.get_lower_bound());
+            row_ub.push_back(constraint.get_upper_bound());
+            rows.push_back(new CoinShallowPackedVector(
+                               constraint.get_variables().size(),
+                               constraint.get_variables().data(),
+                               constraint.get_coefficients().data(),
+                               false));
+        }
+        try {
+            lp_solver->addRows(num_rows,
+                               rows.data(), row_lb.data(), row_ub.data());
+        } catch (CoinError &error) {
+            handle_coin_error(error);
+        }
+        for (CoinPackedVectorBase *row : rows) {
+            delete row;
+        }
+        clear_temporary_data();
+        //has_temporary_constraints_ = false;
+        //is_solved = false;
+    }
+}
+///////////////////////////////////////////////////////////////////
+
 void LPSolver::clear_temporary_constraints() {
     if (has_temporary_constraints_) {
         try {
@@ -297,6 +339,20 @@
     }
 }
 
+void LPSolver::solve_mip() {
+    try {
+        lp_solver->branchAndBound();
+        if (lp_solver->isAbandoned()) {
+            cerr << "Abandoned LP. "
+                 << "Reasons include \"numerical difficulties\" and running out of memory." << endl;
+            utils::exit_with(ExitCode::SEARCH_CRITICAL_ERROR);
+        }
+        is_solved = true;
+    } catch (CoinError &error) {
+        handle_coin_error(error);
+    }
+}
+
 bool LPSolver::has_optimal_solution() const {
     assert(is_solved);
     try {
diff -X lp-recognizer/fd-patch.ignore -ruN fast-downward-original/src/search/lp/lp_solver.h fast-downward/src/search/lp/lp_solver.h
--- fast-downward-original/src/search/lp/lp_solver.h	2019-02-11 14:16:39.000000000 -0200
+++ fast-downward/src/search/lp/lp_solver.h	2019-02-11 14:15:21.000000000 -0200
@@ -67,10 +67,12 @@
     double lower_bound;
     double upper_bound;
     double objective_coefficient;
+    bool is_integer;
 
     LPVariable(double lower_bound,
                double upper_bound,
-               double objective_coefficient);
+               double objective_coefficient,
+               bool is_integer = false);
 };
 
 #ifdef __GNUG__
@@ -85,7 +87,6 @@
 #ifdef USE_LP
     std::unique_ptr<OsiSolverInterface> lp_solver;
 #endif
-
     /*
       Temporary data for assigning a new problem. We keep the vectors
       around to avoid recreating them in every assignment.
@@ -101,6 +102,11 @@
     std::vector<CoinPackedVectorBase *> rows;
     void clear_temporary_data();
 public:
+
+    ///////////////////////////////////////////////////////////////////
+    std::vector<LPConstraint> all_constraints;
+    ///////////////////////////////////////////////////////////////////
+
     LP_METHOD(explicit LPSolver(LPSolverType solver_type))
     /*
       Note that the destructor does not use LP_METHOD because it should not
@@ -115,6 +121,11 @@
                   const std::vector<LPVariable> &variables,
                   const std::vector<LPConstraint> &constraints))
     LP_METHOD(void add_temporary_constraints(const std::vector<LPConstraint> &constraints))
+
+    ///////////////////////////////////////////////////////////////////
+    LP_METHOD(void add_new_constraints(const std::vector<LPConstraint> &constraints))
+    ///////////////////////////////////////////////////////////////////
+
     LP_METHOD(void clear_temporary_constraints())
     LP_METHOD(double get_infinity() const)
 
@@ -126,6 +137,7 @@
     LP_METHOD(void set_variable_upper_bound(int index, double bound))
 
     LP_METHOD(void solve())
+    LP_METHOD(void solve_mip())
 
     /*
       Return true if the solving the LP showed that it is bounded feasible and
diff -X lp-recognizer/fd-patch.ignore -ruN fast-downward-original/src/search/merge_and_shrink/factored_transition_system.cc fast-downward/src/search/merge_and_shrink/factored_transition_system.cc
--- fast-downward-original/src/search/merge_and_shrink/factored_transition_system.cc	2019-02-11 14:16:41.000000000 -0200
+++ fast-downward/src/search/merge_and_shrink/factored_transition_system.cc	2019-02-11 14:13:34.000000000 -0200
@@ -96,7 +96,8 @@
     if (compute_goal_distances && !distances[index]->are_goal_distances_computed()) {
         return false;
     }
-    return transition_systems[index]->are_transitions_sorted_unique();
+    return transition_systems[index]->are_transitions_sorted_unique() &&
+           transition_systems[index]->in_sync_with_label_equivalence_relation();
 }
 
 void FactoredTransitionSystem::assert_all_components_valid() const {
diff -X lp-recognizer/fd-patch.ignore -ruN fast-downward-original/src/search/merge_and_shrink/fts_factory.cc fast-downward/src/search/merge_and_shrink/fts_factory.cc
--- fast-downward-original/src/search/merge_and_shrink/fts_factory.cc	2019-02-11 14:16:41.000000000 -0200
+++ fast-downward/src/search/merge_and_shrink/fts_factory.cc	2019-02-11 14:13:34.000000000 -0200
@@ -10,6 +10,7 @@
 
 #include "../task_proxy.h"
 
+#include "../utils/collections.h"
 #include "../utils/memory.h"
 
 #include <algorithm>
@@ -29,7 +30,8 @@
         vector<int> incorporated_variables;
 
         unique_ptr<LabelEquivalenceRelation> label_equivalence_relation;
-        vector<vector<Transition>> transitions_by_label;
+        vector<vector<int>> label_groups;
+        vector<vector<Transition>> transitions_by_group_id;
         vector<bool> relevant_labels;
         int num_states;
         vector<bool> goal_states;
@@ -38,7 +40,8 @@
             : num_variables(other.num_variables),
               incorporated_variables(move(other.incorporated_variables)),
               label_equivalence_relation(move(other.label_equivalence_relation)),
-              transitions_by_label(move(other.transitions_by_label)),
+              label_groups(move(other.label_groups)),
+              transitions_by_group_id(move(other.transitions_by_group_id)),
               relevant_labels(move(other.relevant_labels)),
               num_states(other.num_states),
               goal_states(move(other.goal_states)),
@@ -53,25 +56,26 @@
     int task_has_conditional_effects;
 
     vector<unique_ptr<Label>> create_labels();
-    void build_label_equivalence_relation(LabelEquivalenceRelation &label_equivalence_relation);
     void build_state_data(VariableProxy var);
     void initialize_transition_system_data(const Labels &labels);
-    void add_transition(int var_no, int label_no,
-                        int src_value, int dest_value);
     bool is_relevant(int var_no, int label_no) const;
     void mark_as_relevant(int var_no, int label_no);
     unordered_map<int, int> compute_preconditions(OperatorProxy op);
     void handle_operator_effect(
-        OperatorProxy op, EffectProxy effect,
+        OperatorProxy op,
+        EffectProxy effect,
         const unordered_map<int, int> &pre_val,
-        vector<bool> &has_effect_on_var);
+        vector<bool> &has_effect_on_var,
+        vector<vector<Transition>> &transitions_by_var);
     void handle_operator_precondition(
-        OperatorProxy op, FactProxy precondition,
-        const vector<bool> &has_effect_on_var);
+        OperatorProxy op,
+        FactProxy precondition,
+        const vector<bool> &has_effect_on_var,
+        vector<vector<Transition>> &transitions_by_var);
     void build_transitions_for_operator(OperatorProxy op);
     void build_transitions_for_irrelevant_ops(VariableProxy variable);
     void build_transitions();
-    vector<unique_ptr<TransitionSystem>> create_transition_systems();
+    vector<unique_ptr<TransitionSystem>> create_transition_systems(const Labels &labels);
     vector<unique_ptr<MergeAndShrinkRepresentation>> create_mas_representations();
     vector<unique_ptr<Distances>> create_distances(
         const vector<unique_ptr<TransitionSystem>> &transition_systems);
@@ -99,25 +103,17 @@
 
 vector<unique_ptr<Label>> FTSFactory::create_labels() {
     vector<unique_ptr<Label>> result;
+    int num_ops = task_proxy.get_operators().size();
+    if (num_ops > 0) {
+        int max_num_labels = 2 * num_ops - 1;
+        result.reserve(max_num_labels);
+    }
     for (OperatorProxy op : task_proxy.get_operators()) {
         result.push_back(utils::make_unique_ptr<Label>(op.get_cost()));
     }
     return result;
 }
 
-void FTSFactory::build_label_equivalence_relation(
-    LabelEquivalenceRelation &label_equivalence_relation) {
-    /*
-      Prepare label_equivalence_relation data structure: add one single-element
-      group for every operator.
-    */
-    int num_labels = task_proxy.get_operators().size();
-    for (int label_no = 0; label_no < num_labels; ++label_no) {
-        // We use the label number as index for transitions of groups.
-        label_equivalence_relation.add_label_group({label_no});
-    }
-}
-
 void FTSFactory::build_state_data(VariableProxy var) {
     int var_id = var.get_id();
     TransitionSystemData &ts_data = transition_system_data_by_var[var_id];
@@ -152,20 +148,12 @@
         TransitionSystemData &ts_data = transition_system_data_by_var[var.get_id()];
         ts_data.num_variables = variables.size();
         ts_data.incorporated_variables.push_back(var.get_id());
-        ts_data.label_equivalence_relation = utils::make_unique_ptr<LabelEquivalenceRelation>(labels);
-        build_label_equivalence_relation(*ts_data.label_equivalence_relation);
-        ts_data.transitions_by_label.resize(labels.get_max_size());
+        ts_data.transitions_by_group_id.reserve(labels.get_max_size());
         ts_data.relevant_labels.resize(num_labels, false);
         build_state_data(var);
     }
 }
 
-void FTSFactory::add_transition(int var_no, int label_no,
-                                int src_value, int dest_value) {
-    transition_system_data_by_var[var_no].transitions_by_label[label_no].push_back(
-        Transition(src_value, dest_value));
-}
-
 bool FTSFactory::is_relevant(int var_no, int label_no) const {
     return transition_system_data_by_var[var_no].relevant_labels[label_no];
 }
@@ -183,8 +171,11 @@
 }
 
 void FTSFactory::handle_operator_effect(
-    OperatorProxy op, EffectProxy effect,
-    const unordered_map<int, int> &pre_val, vector<bool> &has_effect_on_var) {
+    OperatorProxy op,
+    EffectProxy effect,
+    const unordered_map<int, int> &pre_val,
+    vector<bool> &has_effect_on_var,
+    vector<vector<Transition>> &transitions_by_var) {
     int label_no = op.get_id();
     FactProxy fact = effect.get_fact();
     VariableProxy var = fact.get_variable();
@@ -233,7 +224,7 @@
           a condition on var and this condition is not satisfied.
         */
         if (cond_effect_pre_value == -1 || cond_effect_pre_value == value)
-            add_transition(var_no, label_no, value, post_value);
+            transitions_by_var[var_no].emplace_back(value, post_value);
     }
 
     // Handle transitions that occur when the effect does not trigger.
@@ -247,7 +238,7 @@
               fails to trigger if this condition is false.
             */
             if (has_other_effect_cond || value != cond_effect_pre_value)
-                add_transition(var_no, label_no, value, value);
+                transitions_by_var[var_no].emplace_back(value, value);
         }
         task_has_conditional_effects = true;
     }
@@ -255,13 +246,15 @@
 }
 
 void FTSFactory::handle_operator_precondition(
-    OperatorProxy op, FactProxy precondition,
-    const vector<bool> &has_effect_on_var) {
+    OperatorProxy op,
+    FactProxy precondition,
+    const vector<bool> &has_effect_on_var,
+    vector<vector<Transition>> &transitions_by_var) {
     int label_no = op.get_id();
     int var_no = precondition.get_variable().get_id();
     if (!has_effect_on_var[var_no]) {
         int value = precondition.get_value();
-        add_transition(var_no, label_no, value, value);
+        transitions_by_var[var_no].emplace_back(value, value);
         mark_as_relevant(var_no, label_no);
     }
 }
@@ -273,17 +266,60 @@
       - Add transitions induced by op in these transition systems.
     */
     unordered_map<int, int> pre_val = compute_preconditions(op);
+    int num_variables = task_proxy.get_variables().size();
     vector<bool> has_effect_on_var(task_proxy.get_variables().size(), false);
+    vector<vector<Transition>> transitions_by_var(num_variables);
 
     for (EffectProxy effect : op.get_effects())
-        handle_operator_effect(op, effect, pre_val, has_effect_on_var);
+        handle_operator_effect(op, effect, pre_val, has_effect_on_var, transitions_by_var);
 
     /*
       We must handle preconditions *after* effects because handling
       the effects sets has_effect_on_var.
     */
     for (FactProxy precondition : op.get_preconditions())
-        handle_operator_precondition(op, precondition, has_effect_on_var);
+        handle_operator_precondition(op, precondition, has_effect_on_var, transitions_by_var);
+
+    int label_no = op.get_id();
+    for (int var_no = 0; var_no < num_variables; ++var_no) {
+        if (!is_relevant(var_no, label_no)) {
+            /*
+              We do not want to add transitions of irrelevant labels here,
+              since they are handled together in a separate step.
+            */
+            continue;
+        }
+        vector<Transition> &transitions = transitions_by_var[var_no];
+        /*
+          TODO: Our method for generating transitions is only guarantueed
+          to generate sorted and unique transitions if the task has no
+          conditional effects.
+        */
+        if (task_has_conditional_effects) {
+            utils::sort_unique(transitions);
+        } else {
+            assert(utils::is_sorted_unique(transitions));
+        }
+
+        vector<vector<Transition>> &existing_transitions_by_group_id =
+            transition_system_data_by_var[var_no].transitions_by_group_id;
+        vector<vector<int>> &label_groups = transition_system_data_by_var[var_no].label_groups;
+        assert(existing_transitions_by_group_id.size() == label_groups.size());
+        bool found_locally_equivalent_label_group = false;
+        for (size_t group_id = 0; group_id < existing_transitions_by_group_id.size(); ++group_id) {
+            const vector<Transition> &group_transitions = existing_transitions_by_group_id[group_id];
+            if (transitions == group_transitions) {
+                label_groups[group_id].push_back(label_no);
+                found_locally_equivalent_label_group = true;
+                break;
+            }
+        }
+
+        if (!found_locally_equivalent_label_group) {
+            existing_transitions_by_group_id.push_back(move(transitions));
+            label_groups.push_back({label_no});
+        }
+    }
 }
 
 void FTSFactory::build_transitions_for_irrelevant_ops(VariableProxy variable) {
@@ -291,50 +327,43 @@
     int num_states = variable.get_domain_size();
     int num_labels = task_proxy.get_operators().size();
 
-    // Make all irrelevant labels explicit.
+    // Collect all irrelevant labels for this variable.
+    vector<int> irrelevant_labels;
     for (int label_no = 0; label_no < num_labels; ++label_no) {
         if (!is_relevant(var_no, label_no)) {
-            for (int state = 0; state < num_states; ++state)
-                add_transition(var_no, label_no, state, state);
+            irrelevant_labels.push_back(label_no);
         }
     }
+
+    TransitionSystemData &ts_data = transition_system_data_by_var[var_no];
+    if (!irrelevant_labels.empty()) {
+        vector<Transition> transitions;
+        transitions.reserve(num_states);
+        for (int state = 0; state < num_states; ++state)
+            transitions.emplace_back(state, state);
+        ts_data.label_groups.push_back(move(irrelevant_labels));
+        ts_data.transitions_by_group_id.push_back(move(transitions));
+    }
 }
 
 void FTSFactory::build_transitions() {
     /*
-      - Add all transitions.
+      - Compute all transitions of all operators for all variables, grouping
+        transitions of locally equivalent labels for a given variable.
       - Computes relevant operator information as a side effect.
     */
     for (OperatorProxy op : task_proxy.get_operators())
         build_transitions_for_operator(op);
 
+    /*
+      Compute transitions of irrelevant operators for each variable only
+      once and put the labels into a single label group.
+    */
     for (VariableProxy variable : task_proxy.get_variables())
         build_transitions_for_irrelevant_ops(variable);
-
-    if (task_has_conditional_effects) {
-        /*
-          TODO: Our method for generating transitions is only guarantueed
-          to generate sorted and unique transitions if the task has no
-          conditional effects. We could replace the instance variable by
-          a call to has_conditional_effects(task_proxy).
-          Generally, the questions is whether we rely on sorted transitions
-          anyway.
-        */
-        int num_variables = task_proxy.get_variables().size();
-        for (int var_no = 0; var_no < num_variables; ++var_no) {
-            vector<vector<Transition>> &transitions_by_label =
-                transition_system_data_by_var[var_no].transitions_by_label;
-            for (vector<Transition> &transitions : transitions_by_label) {
-                sort(transitions.begin(), transitions.end());
-                transitions.erase(unique(transitions.begin(),
-                                         transitions.end()),
-                                  transitions.end());
-            }
-        }
-    }
 }
 
-vector<unique_ptr<TransitionSystem>> FTSFactory::create_transition_systems() {
+vector<unique_ptr<TransitionSystem>> FTSFactory::create_transition_systems(const Labels &labels) {
     // Create the actual TransitionSystem objects.
     int num_variables = task_proxy.get_variables().size();
 
@@ -343,18 +372,21 @@
     assert(num_variables >= 1);
     result.reserve(num_variables * 2 - 1);
 
-    const bool compute_label_equivalence_relation = true;
     for (int var_no = 0; var_no < num_variables; ++var_no) {
         TransitionSystemData &ts_data = transition_system_data_by_var[var_no];
+        /* Construct the label equivalence relation from the previously
+           computed label groups. */
+        ts_data.label_equivalence_relation =
+            utils::make_unique_ptr<LabelEquivalenceRelation>(
+                labels, ts_data.label_groups);
         result.push_back(utils::make_unique_ptr<TransitionSystem>(
                              ts_data.num_variables,
                              move(ts_data.incorporated_variables),
                              move(ts_data.label_equivalence_relation),
-                             move(ts_data.transitions_by_label),
+                             move(ts_data.transitions_by_group_id),
                              ts_data.num_states,
                              move(ts_data.goal_states),
-                             ts_data.init_state,
-                             compute_label_equivalence_relation
+                             ts_data.init_state
                              ));
     }
     return result;
@@ -407,7 +439,7 @@
     initialize_transition_system_data(*labels);
     build_transitions();
     vector<unique_ptr<TransitionSystem>> transition_systems =
-        create_transition_systems();
+        create_transition_systems(*labels);
     vector<unique_ptr<MergeAndShrinkRepresentation>> mas_representations =
         create_mas_representations();
     vector<unique_ptr<Distances>> distances =
diff -X lp-recognizer/fd-patch.ignore -ruN fast-downward-original/src/search/merge_and_shrink/label_equivalence_relation.cc fast-downward/src/search/merge_and_shrink/label_equivalence_relation.cc
--- fast-downward-original/src/search/merge_and_shrink/label_equivalence_relation.cc	2019-02-11 14:16:41.000000000 -0200
+++ fast-downward/src/search/merge_and_shrink/label_equivalence_relation.cc	2019-02-11 14:13:34.000000000 -0200
@@ -7,10 +7,16 @@
 using namespace std;
 
 namespace merge_and_shrink {
-LabelEquivalenceRelation::LabelEquivalenceRelation(const Labels &labels)
+LabelEquivalenceRelation::LabelEquivalenceRelation(
+    const Labels &labels, const vector<vector<int>> &label_groups)
     : labels(labels) {
+    /* In the worst case, each label forms a singleton group, and thus with
+       label reduction, we could have labels.get_max_size() many groups. */
     grouped_labels.reserve(labels.get_max_size());
     label_to_positions.resize(labels.get_max_size());
+    for (const vector<int> &label_group : label_groups) {
+        add_label_group(label_group);
+    }
 }
 
 LabelEquivalenceRelation::LabelEquivalenceRelation(
@@ -19,11 +25,7 @@
       /* We copy label_to_positions to have identical vectors even on
       "unused" positions (for label numbers that do not exist any more). */
       label_to_positions(other.label_to_positions) {
-    /*
-      We need to reserve space for the potential maximum number of labels to
-      ensure that no move occurs in grouped_labels. Otherwise, iterators to
-      elements of list<int> of LabelGroup could become invalid!
-    */
+    // For the reserve call, see the comment in the constructor above.
     grouped_labels.reserve(labels.get_max_size());
     for (size_t other_group_id = 0;
          other_group_id < other.grouped_labels.size();
diff -X lp-recognizer/fd-patch.ignore -ruN fast-downward-original/src/search/merge_and_shrink/label_equivalence_relation.h fast-downward/src/search/merge_and_shrink/label_equivalence_relation.h
--- fast-downward-original/src/search/merge_and_shrink/label_equivalence_relation.h	2019-02-11 14:16:41.000000000 -0200
+++ fast-downward/src/search/merge_and_shrink/label_equivalence_relation.h	2019-02-11 14:13:34.000000000 -0200
@@ -67,23 +67,15 @@
 
     const Labels &labels;
 
-    /*
-      NOTE: it is somewhat dangerous to use lists inside vectors and storing
-      iterators to these lists, because whenever the vector needs to be
-      resized, iterators to these lists may become invalid. In the constructor,
-      we thus make sure to reserve enough memory so reallocation is never needed.
-    */
     std::vector<LabelGroup> grouped_labels;
-    // maps each label to its group's ID and its iterator within the group.
+    /* Maps each label to its group's ID (index in grouped_labels) and its
+       iterator within the group. */
     std::vector<std::pair<int, LabelIter>> label_to_positions;
 
     void add_label_to_group(int group_id, int label_no);
 public:
-    /*
-      Constructs an empty label equivalence relation. It can be filled using
-      the public add_label_group method below.
-    */
-    explicit LabelEquivalenceRelation(const Labels &labels);
+    LabelEquivalenceRelation(
+        const Labels &labels, const std::vector<std::vector<int>> &label_groups);
     /*
       NOTE: we need a custom copy constructor here because we need to fill
       label_to_positions with correct LabelIter objects that point to the
diff -X lp-recognizer/fd-patch.ignore -ruN fast-downward-original/src/search/merge_and_shrink/transition_system.cc fast-downward/src/search/merge_and_shrink/transition_system.cc
--- fast-downward-original/src/search/merge_and_shrink/transition_system.cc	2019-02-11 14:16:41.000000000 -0200
+++ fast-downward/src/search/merge_and_shrink/transition_system.cc	2019-02-11 14:13:34.000000000 -0200
@@ -81,22 +81,19 @@
     int num_variables,
     vector<int> &&incorporated_variables,
     unique_ptr<LabelEquivalenceRelation> &&label_equivalence_relation,
-    vector<vector<Transition>> &&transitions_by_label,
+    vector<vector<Transition>> &&transitions_by_group_id,
     int num_states,
     vector<bool> &&goal_states,
-    int init_state,
-    bool compute_label_equivalence_relation)
+    int init_state)
     : num_variables(num_variables),
       incorporated_variables(move(incorporated_variables)),
       label_equivalence_relation(move(label_equivalence_relation)),
-      transitions_by_group_id(move(transitions_by_label)),
+      transitions_by_group_id(move(transitions_by_group_id)),
       num_states(num_states),
       goal_states(move(goal_states)),
       init_state(init_state) {
-    if (compute_label_equivalence_relation) {
-        compute_locally_equivalent_labels();
-    }
     assert(are_transitions_sorted_unique());
+    assert(in_sync_with_label_equivalence_relation());
 }
 
 TransitionSystem::TransitionSystem(const TransitionSystem &other)
@@ -133,9 +130,9 @@
         ts1.incorporated_variables.begin(), ts1.incorporated_variables.end(),
         ts2.incorporated_variables.begin(), ts2.incorporated_variables.end(),
         back_inserter(incorporated_variables));
-    unique_ptr<LabelEquivalenceRelation> label_equivalence_relation =
-        utils::make_unique_ptr<LabelEquivalenceRelation>(labels);
-    vector<vector<Transition>> transitions_by_group_id(labels.get_max_size());
+    vector<vector<int>> label_groups;
+    vector<vector<Transition>> transitions_by_group_id;
+    transitions_by_group_id.reserve(labels.get_max_size());
 
     int ts1_size = ts1.get_size();
     int ts2_size = ts2.get_size();
@@ -180,7 +177,7 @@
         // refinements of group1.
 
         // Now create the new groups together with their transitions.
-        for (const auto &bucket : buckets) {
+        for (auto &bucket : buckets) {
             const vector<Transition> &transitions2 =
                 ts2.get_transitions_for_group_id(bucket.first);
 
@@ -203,13 +200,13 @@
             }
 
             // Create a new group if the transitions are not empty
-            const vector<int> &new_labels = bucket.second;
+            vector<int> &new_labels = bucket.second;
             if (new_transitions.empty()) {
                 dead_labels.insert(dead_labels.end(), new_labels.begin(), new_labels.end());
             } else {
                 sort(new_transitions.begin(), new_transitions.end());
-                int new_index = label_equivalence_relation->add_label_group(new_labels);
-                transitions_by_group_id[new_index] = move(new_transitions);
+                label_groups.push_back(move(new_labels));
+                transitions_by_group_id.push_back(move(new_transitions));
             }
         }
     }
@@ -222,11 +219,16 @@
       All dead labels should form one single label group.
     */
     if (!dead_labels.empty()) {
+        label_groups.push_back(move(dead_labels));
         // Dead labels have empty transitions
-        label_equivalence_relation->add_label_group(dead_labels);
+        transitions_by_group_id.emplace_back();
     }
 
-    const bool compute_label_equivalence_relation = false;
+    assert(transitions_by_group_id.size() == label_groups.size());
+
+    unique_ptr<LabelEquivalenceRelation> label_equivalence_relation =
+        utils::make_unique_ptr<LabelEquivalenceRelation>(labels, label_groups);
+
     return utils::make_unique_ptr<TransitionSystem>(
         num_variables,
         move(incorporated_variables),
@@ -234,8 +236,7 @@
         move(transitions_by_group_id),
         num_states,
         move(goal_states),
-        init_state,
-        compute_label_equivalence_relation
+        init_state
         );
 }
 
@@ -252,8 +253,7 @@
                  group_id2 < label_equivalence_relation->get_size(); ++group_id2) {
                 if (!label_equivalence_relation->is_empty_group(group_id2)) {
                     vector<Transition> &transitions2 = transitions_by_group_id[group_id2];
-                    if ((transitions1.empty() && transitions2.empty())
-                        || transitions1 == transitions2) {
+                    if (transitions1 == transitions2) {
                         label_equivalence_relation->move_group_into_group(
                             group_id2, group_id1);
                         utils::release_vector_memory(transitions2);
@@ -269,6 +269,7 @@
     const vector<int> &abstraction_mapping,
     Verbosity verbosity) {
     assert(are_transitions_sorted_unique());
+    assert(in_sync_with_label_equivalence_relation());
 
     int new_num_states = state_equivalence_relation.size();
     assert(new_num_states < num_states);
@@ -327,12 +328,14 @@
     }
 
     assert(are_transitions_sorted_unique());
+    assert(in_sync_with_label_equivalence_relation());
 }
 
 void TransitionSystem::apply_label_reduction(
     const vector<pair<int, vector<int>>> &label_mapping,
     bool only_equivalent_labels) {
     assert(are_transitions_sorted_unique());
+    assert(in_sync_with_label_equivalence_relation());
 
     /*
       We iterate over the given label mapping, treating every new label and
@@ -368,10 +371,10 @@
           updating label_equivalence_relation, because after updating it,
           we cannot find out the group ID of reduced labels anymore.
         */
-        unordered_map<int, vector<Transition>> new_label_to_transitions;
+        vector<vector<Transition>> new_transitions;
+        new_transitions.reserve(label_mapping.size());
         unordered_set<int> affected_group_ids;
         for (const pair<int, vector<int>> &mapping: label_mapping) {
-            int new_label_no = mapping.first;
             const vector<int> &old_label_nos = mapping.second;
             assert(old_label_nos.size() >= 2);
             unordered_set<int> seen_group_ids;
@@ -384,9 +387,10 @@
                     new_label_transitions.insert(transitions.begin(), transitions.end());
                 }
             }
-            new_label_to_transitions[new_label_no] =
-                vector<Transition>(new_label_transitions.begin(), new_label_transitions.end());
+            new_transitions.emplace_back(
+                new_label_transitions.begin(), new_label_transitions.end());
         }
+        assert(label_mapping.size() == new_transitions.size());
 
         /*
            Apply all label mappings to label_equivalence_relation. This needs
@@ -397,12 +401,28 @@
         */
         label_equivalence_relation->apply_label_mapping(label_mapping, &affected_group_ids);
 
-        // Go over the new transitions and add them at the correct position.
-        for (auto &label_and_transitions : new_label_to_transitions) {
-            int new_label_no = label_and_transitions.first;
-            vector<Transition> &transitions = label_and_transitions.second;
+        /*
+          Go over the transitions of new labels and add them at the correct
+          position.
+
+          NOTE: it is important that this happens in increasing order of label
+          numbers to ensure that transitions_by_group_id are synchronized with
+          label groups of label_equivalence_relation.
+        */
+        for (size_t i = 0; i < label_mapping.size(); ++i) {
+            int new_label_no = label_mapping[i].first;
+            vector<Transition> &transitions = new_transitions[i];
             int new_group_id = label_equivalence_relation->get_group_id(new_label_no);
-            transitions_by_group_id[new_group_id] = move(transitions);
+            if (!utils::in_bounds(new_group_id, transitions_by_group_id)) {
+                /* Labels reduced to new_label_no were not locally equivalent
+                   and hence assigned to a new group. */
+                assert(new_group_id == static_cast<int>(transitions_by_group_id.size()));
+                transitions_by_group_id.push_back(move(transitions));
+            } else {
+                /* Labels reduced to new_label_no were locally equivalent before
+                   and hence the new label is part of the same group. */
+                transitions_by_group_id[new_group_id] = move(transitions);
+            }
         }
 
         // Go over all affected group IDs and remove their transitions if the
@@ -417,6 +437,7 @@
     }
 
     assert(are_transitions_sorted_unique());
+    assert(in_sync_with_label_equivalence_relation());
 }
 
 string TransitionSystem::tag() const {
@@ -433,6 +454,11 @@
     return true;
 }
 
+bool TransitionSystem::in_sync_with_label_equivalence_relation() const {
+    return label_equivalence_relation->get_size() ==
+           static_cast<int>(transitions_by_group_id.size());
+}
+
 bool TransitionSystem::is_solvable(const Distances &distances) const {
     if (init_state == PRUNED_STATE) {
         return false;
diff -X lp-recognizer/fd-patch.ignore -ruN fast-downward-original/src/search/merge_and_shrink/transition_system.h fast-downward/src/search/merge_and_shrink/transition_system.h
--- fast-downward-original/src/search/merge_and_shrink/transition_system.h	2019-02-11 14:16:41.000000000 -0200
+++ fast-downward/src/search/merge_and_shrink/transition_system.h	2019-02-11 14:13:34.000000000 -0200
@@ -134,11 +134,10 @@
         int num_variables,
         std::vector<int> &&incorporated_variables,
         std::unique_ptr<LabelEquivalenceRelation> &&label_equivalence_relation,
-        std::vector<std::vector<Transition>> &&transitions_by_label,
+        std::vector<std::vector<Transition>> &&transitions_by_group_id,
         int num_states,
         std::vector<bool> &&goal_states,
-        int init_state,
-        bool compute_label_equivalence_relation);
+        int init_state);
     TransitionSystem(const TransitionSystem &other);
     ~TransitionSystem();
     /*
@@ -200,6 +199,7 @@
       sorted (by source, by target) and there are no duplicates.
     */
     bool are_transitions_sorted_unique() const;
+    bool in_sync_with_label_equivalence_relation() const;
 
     bool is_solvable(const Distances &distances) const;
     void dump_dot_graph() const;
diff -X lp-recognizer/fd-patch.ignore -ruN fast-downward-original/src/search/operator_counting/oc_single_shot_heuristic.cc fast-downward/src/search/operator_counting/oc_single_shot_heuristic.cc
--- fast-downward-original/src/search/operator_counting/oc_single_shot_heuristic.cc	1969-12-31 21:00:00.000000000 -0300
+++ fast-downward/src/search/operator_counting/oc_single_shot_heuristic.cc	2019-02-07 18:16:12.000000000 -0200
@@ -0,0 +1,346 @@
+#include "oc_single_shot_heuristic.h"
+
+#include "constraint_generator.h"
+
+#include "../option_parser.h"
+#include "../plugin.h"
+
+#include "../utils/markup.h"
+
+#include <cmath>
+#include <fstream>
+
+#include <algorithm> 
+#include <cctype>
+#include <locale>
+
+// trim from start (in place)
+static inline void ltrim(std::string &s) {
+    s.erase(s.begin(), std::find_if(s.begin(), s.end(), [](int ch) {
+        return !std::isspace(ch);
+    }));
+}
+
+// trim from end (in place)
+static inline void rtrim(std::string &s) {
+    s.erase(std::find_if(s.rbegin(), s.rend(), [](int ch) {
+        return !std::isspace(ch);
+    }).base(), s.end());
+}
+
+// trim from both ends (in place)
+static inline void trim(std::string &s) {
+    ltrim(s);
+    rtrim(s);
+}
+
+using namespace std;
+
+namespace operator_counting {
+
+OCSingleShotHeuristic::OCSingleShotHeuristic(const Options &opts)
+    : Heuristic(opts),
+      constraint_generators(
+          opts.get_list<shared_ptr<ConstraintGenerator>>("constraint_generators")),
+      lp_solver(lp::LPSolverType(opts.get_enum("lpsolver"))),
+      enforce_observations(opts.get("enforce_observations",false)),
+      soft_constraints(opts.get("soft_constraints",false)),
+      op_indexes(),
+      observations(){
+
+    load_observations();
+
+    vector<lp::LPVariable> variables;
+    double infinity = lp_solver.get_infinity();
+    for (OperatorProxy op : task_proxy.get_operators()) {
+        int op_cost = op.get_cost();
+        if (soft_constraints == false) {
+            variables.push_back(lp::LPVariable(0, infinity, op_cost));
+        } else { // Add variables to create soft constraints
+            variables.push_back(lp::LPVariable(0, infinity, 10000*op_cost));
+        }
+    }
+
+    vector<lp::LPConstraint> constraints;
+    for (const auto &generator : constraint_generators) {
+        generator->initialize_constraints(task, constraints, infinity);
+    }
+
+    map_operators(false);
+
+    if (soft_constraints == true) {
+        add_observation_soft_constraints(variables, constraints);
+    }
+    if(enforce_observations == true) {
+        enforce_observation_constraints(constraints);
+    }
+
+    show_variables_and_objective(variables, false);
+
+    lp_solver.load_problem(lp::LPObjectiveSense::MINIMIZE, variables, constraints);
+}
+
+void OCSingleShotHeuristic::map_operators(bool show) {
+    if (show == true) {
+        cout << endl << string(80, '*') << endl;
+        cout << "# Mapping X -> op: " << endl;
+    }
+    int i = 0;
+    for (OperatorProxy op : task_proxy.get_operators()) {
+        // Caching operator variable indexes
+        std::string op_name (op.get_name());
+        for (size_t i = 0; i< op.get_name().size(); ++i) {
+            op_name[i] = tolower(op_name.c_str()[i]);
+        }
+
+        op_indexes[op_name] = i;
+        if (show == true) {
+            cout << "["<< op_name<< "]: " << op_indexes[op_name] << endl;
+        }
+        i++;
+    }
+    if (show == true) {
+        cout << string(80, '*') << endl << endl;
+    }
+}
+
+void OCSingleShotHeuristic::show_variables_and_objective(const std::vector<lp::LPVariable> &variables, bool show) {
+    if (show == true) {
+        cout << endl << string(80, '*') << endl;
+        cout << "# Variables(" << variables.size() << "): " << endl;
+        for (int i = 0; i < (int) variables.size(); ++i) {
+            cout << "X[" << i << "] = Variable('X_" << i << "'";
+            cout << ", lb=" << variables[i].lower_bound;
+            cout << ", ub=" << variables[i].upper_bound;
+            cout << ", cost[" << i << "] = " << variables[i].objective_coefficient << endl;
+        }
+        cout << string(80, '*') << endl << endl;
+
+        cout << endl << string(80, '*') << endl;
+        cout << "# Objective function: " << endl;
+        cout << "obj = Objective(";
+        for (int i = 0; i < (int) variables.size(); ++i) {
+            cout << "cost[" << i << "] * X[" << i << "]";
+            if (i < (int) variables.size() - 1) {
+                cout << " + ";
+            }
+        }
+        cout << ", direction='min')" << endl;
+        cout << string(80, '*') << endl << endl;
+    }
+}
+
+void OCSingleShotHeuristic::add_observation_soft_constraints(std::vector<lp::LPVariable> &variables, std::vector<lp::LPConstraint> &constraints) {
+    double infinity = lp_solver.get_infinity();
+    cout << endl << string(80, '*') << endl;
+    // Adding constraints
+    cout << "Add soft constraints" << endl;
+    for(vector<string>::iterator it = observations.begin() ; it != observations.end(); ++it) {
+        variables.push_back(lp::LPVariable(-infinity, infinity, -1.0));
+
+        cout << "Adding soft constraint on (" << (*it) << "), index " << std::to_string(op_indexes[*it]) << endl;
+        lp::LPConstraint constraint(0.0, 0.0);
+        constraint.insert(op_indexes[*it], 1.0);
+        constraint.insert(variables.size() - 1, -1.0);
+
+        cout << "X[" << op_indexes[*it] << "] = Variable('X_" << op_indexes[*it]  << "'";
+        cout << ", lb=" << variables[op_indexes[*it]].lower_bound;
+        cout << ", ub=" << variables[op_indexes[*it]].upper_bound;
+        cout << ", cost[" << op_indexes[*it] << "] = " << variables[op_indexes[*it]].objective_coefficient << endl;
+
+        cout << "X[" << variables.size() - 1 << "] = Variable('X_" << variables.size() - 1  << "'";
+        cout << ", lb=" << variables[variables.size() - 1].lower_bound;
+        cout << ", ub=" << variables[variables.size() - 1].upper_bound;
+        cout << ", cost[" << variables.size() - 1 << "] = " << variables[variables.size() - 1].objective_coefficient << endl;
+
+        cout << "constraint variables: " << constraint.get_variables()[0];
+        cout << ", " << constraint.get_variables()[1] << " - ";
+        cout << "constraint coefficients: " << constraint.get_coefficients()[0];
+        cout << ", " << constraint.get_coefficients()[1] << endl << endl;
+        constraints.push_back(constraint);
+    }
+    cout << endl << string(80, '*') << endl;
+}
+
+void OCSingleShotHeuristic::load_observations() {
+    // Read observations from file
+    cout << endl << string(80, '*') << endl;
+    cout << std::endl << "Load observations" << std::endl;
+    ifstream obs_file;
+    obs_file.open("obs.dat");
+    if(obs_file.is_open()){
+        while(!obs_file.eof()) {
+            string obs;
+            getline(obs_file, obs);
+            trim(obs);
+            if(!obs.empty() && obs[0]!=';') {
+                obs = obs.substr(1,obs.length()-2);
+                std::string obs_name (obs);
+                for (size_t i = 0; i< obs.size(); ++i) {
+                    obs_name[i] = tolower(obs.c_str()[i]);
+                }
+                cout << "Observation: " << obs_name << endl;
+                observations.push_back(obs_name);
+            }
+        }
+    }
+    cout << endl << string(80, '*') << endl;
+
+    obs_file.close();
+}
+
+void OCSingleShotHeuristic::enforce_observation_constraints(std::vector<lp::LPConstraint> &constraints) {
+    cout << endl << string(80, '*') << endl;
+    // Adding constraints
+    std::cout << "Enforcing observation constraints" << std::endl;
+
+    double infinity = lp_solver.get_infinity();
+    for(vector<string>::iterator it = observations.begin() ; it != observations.end(); ++it) {
+        lp::LPConstraint constraint(1, infinity);
+
+        cout << "constraint " << (*it) << ": " << std::to_string(op_indexes[*it]) << endl;
+        constraint.insert(op_indexes[*it], 1);
+        constraints.push_back(constraint);
+    }
+    cout << endl << string(80, '*') << endl;
+}
+
+void OCSingleShotHeuristic::output_results(int result) {
+    cout << endl << string(80, '*') << endl;
+    vector<double> solution = lp_solver.extract_solution();
+    for (int i = 0; i < (int) solution.size(); ++i) {
+        cout << "X[" << i << "] = " << solution[i] << endl;
+    }
+    std::cout << "# observations in solution (" << observations.size() << "): " << std::endl;
+    double sat_observations = 0.0;
+    for(vector<string>::iterator it = observations.begin() ; it != observations.end(); ++it) {
+        cout << (*it) << ": " << solution[op_indexes[*it]] << endl;
+        sat_observations += solution[op_indexes[*it]];
+    }
+    cout << "# sat observations: " << sat_observations << endl;
+    cout << "# h-value: " << result << endl;
+    cout << string(80, '*') << endl;
+
+    cout << endl << string(80, '*') << endl;
+
+    cout << "Writing results" << endl;
+    ofstream results;
+    //cout << "Writing results" << endl;
+    results.open("ocsingleshot_heuristic_result.dat");
+    results << "-- ";
+    results << endl << result << endl;
+    // Printing counts
+    int var_i=0;
+    vector<double> counts = lp_solver.extract_solution();
+    for (OperatorProxy op : task_proxy.get_operators()) {
+        // cout << "(" << op.get_name() << ") = " << counts[var_i] << endl;
+        if (counts[var_i] > 0 ) {
+            results << "(" << op.get_name() << ") = " << counts[var_i] << endl;
+        }
+        var_i++;
+    }
+
+    results.flush();
+    results.close();
+}
+
+OCSingleShotHeuristic::~OCSingleShotHeuristic() {
+}
+
+int OCSingleShotHeuristic::compute_heuristic(const GlobalState &global_state) {
+    State state = convert_global_state(global_state);
+    return compute_heuristic(state);
+}
+
+int OCSingleShotHeuristic::compute_heuristic(const State &state) {
+    assert(!lp_solver.has_temporary_constraints());
+    for (const auto &generator : constraint_generators) {
+          bool dead_end = generator->update_constraints(state, lp_solver);
+        if (dead_end) {
+            lp_solver.clear_temporary_constraints();
+            return DEAD_END;
+        }
+    }
+
+    int result;
+    lp_solver.solve();
+    if (lp_solver.has_optimal_solution()) {
+        double epsilon = 0.01;
+        double objective_value = lp_solver.get_objective_value();
+        result = ceil(objective_value - epsilon);
+        
+    } else {
+        result = DEAD_END;
+    }
+
+    lp_solver.print_statistics();
+
+    ///////////////////////////////////////////////////////////////////
+    output_results(result);
+
+    if(result == DEAD_END)
+        exit(EXIT_FAILURE);
+    else
+        exit(EXIT_SUCCESS);
+    ///////////////////////////////////////////////////////////////////
+
+    lp_solver.clear_temporary_constraints();
+    return result;
+}
+
+static shared_ptr<Heuristic> _parse(OptionParser &parser) {
+    parser.document_synopsis(
+        "Operator counting heuristic, single shot call for recognition",
+        "An operator counting heuristic computes a linear program (LP) in each "
+        "state. The LP has one variable Count_o for each operator o that "
+        "represents how often the operator is used in a plan. Operator "
+        "counting constraints are linear constraints over these varaibles that "
+        "are guaranteed to have a solution with Count_o = occurrences(o, pi) "
+        "for every plan pi. Minimizing the total cost of operators subject to "
+        "some operator counting constraints is an admissible heuristic. "
+        "For details, see" + utils::format_paper_reference( // TODO - Change this for our paper
+            {"Florian Pommerening", "Gabriele Roeger", "Malte Helmert",
+             "Blai Bonet"},
+            "LP-based Heuristics for Cost-optimal Planning",
+            "http://www.aaai.org/ocs/index.php/ICAPS/ICAPS14/paper/view/7892/8031",
+            "Proceedings of the Twenty-Fourth International Conference"
+            " on Automated Planning and Scheduling (ICAPS 2014)",
+            "226-234",
+            "AAAI Press 2014"));
+
+    parser.document_language_support("action costs", "supported");
+    parser.document_language_support(
+        "conditional effects",
+        "not supported (the heuristic supports them in theory, but none of "
+        "the currently implemented constraint generators do)");
+    parser.document_language_support(
+        "axioms",
+        "not supported (the heuristic supports them in theory, but none of "
+        "the currently implemented constraint generators do)");
+    parser.document_property("admissible", "yes");
+    parser.document_property(
+        "consistent",
+        "yes, if all constraint generators represent consistent heuristics");
+    parser.document_property("safe", "yes");
+    // TODO: prefer operators that are non-zero in the solution.
+    parser.document_property("preferred operators", "no");
+
+    parser.add_list_option<shared_ptr<ConstraintGenerator>>(
+        "constraint_generators",
+        "methods that generate constraints over operator counting variables");
+    parser.add_option<bool>("enforce_observations", "whether or not to enforce constraints on observations");
+    parser.add_option<bool>("soft_constraints", "whether or not to use observations as soft constraints");
+    lp::add_lp_solver_option_to_parser(parser);
+    Heuristic::add_options_to_parser(parser);
+    Options opts = parser.parse();
+    if (parser.help_mode())
+        return nullptr;
+    opts.verify_list_non_empty<shared_ptr<ConstraintGenerator>>(
+        "constraint_generators");
+    if (parser.dry_run())
+        return nullptr;
+    return make_shared<OCSingleShotHeuristic>(opts);
+}
+
+static Plugin<Evaluator> _plugin("ocsingleshot", _parse);
+}
diff -X lp-recognizer/fd-patch.ignore -ruN fast-downward-original/src/search/operator_counting/oc_single_shot_heuristic.h fast-downward/src/search/operator_counting/oc_single_shot_heuristic.h
--- fast-downward-original/src/search/operator_counting/oc_single_shot_heuristic.h	1969-12-31 21:00:00.000000000 -0300
+++ fast-downward/src/search/operator_counting/oc_single_shot_heuristic.h	2019-01-17 17:20:02.000000000 -0200
@@ -0,0 +1,41 @@
+#ifndef OPERATOR_COUNTING_OPERATOR_COUNTING_HEURISTIC_H
+#define OPERATOR_COUNTING_OPERATOR_COUNTING_HEURISTIC_H
+
+#include "../heuristic.h"
+
+#include "../lp/lp_solver.h"
+
+#include <memory>
+#include <vector>
+#include <string>
+
+namespace options {
+class Options;
+}
+
+namespace operator_counting {
+class ConstraintGenerator;
+
+class OCSingleShotHeuristic : public Heuristic {
+    std::vector<std::shared_ptr<ConstraintGenerator>> constraint_generators;
+    lp::LPSolver lp_solver;
+    bool enforce_observations;
+    bool soft_constraints;
+    std::unordered_map<std::string,int> op_indexes;
+    std::vector<std::string> observations;
+protected:
+    virtual int compute_heuristic(const GlobalState &global_state) override;
+    int compute_heuristic(const State &state);
+    void load_observations();
+    void enforce_observation_constraints(std::vector<lp::LPConstraint> &constraints);
+    void add_observation_soft_constraints(std::vector<lp::LPVariable> &variables, std::vector<lp::LPConstraint> &constraints);
+    void output_results(int result);
+public:
+    explicit OCSingleShotHeuristic(const options::Options &opts);
+    ~OCSingleShotHeuristic();
+    void map_operators(bool show = false);
+    void show_variables_and_objective(const std::vector<lp::LPVariable> &variables, bool show = false);
+};
+}
+
+#endif
