# Goal Recognition via LP-Constraints

[![Build Status](https://travis-ci.com/pucrs-automated-planning/lp-recognizer.svg?token=wcNhPPzeYu4Vp7Wds6rN&branch=master)](https://travis-ci.com/pucrs-automated-planning/lp-recognizer)

A goal recognizer that uses Linear Programming Heuristics from Automated planning to compute most likely goals.

## Installation

LP-Recognizer is written mostly in Python, but we rely heavily in the constraints generated by the heurstic functions in the [Fast Downward](http://www.fast-downward.org) planner, so configuration is a two-step process.

1. Configure [IBM CPLEX](https://www.ibm.com/products/ilog-cplex-optimization-studio) (we need this to enable the OperatorCounting heuristic plugin in Fast Downward) 
2. Download and configure [Fast Downward](http://www.fast-downward.org/ObtainingAndRunningFastDownward) and apply a patch with our modifications for it.

We use a customized build of Fast Downward, so we provide automated scripts to download and build the requirements for a successful compilation (since there is a sign up required for CPLEX, we cannot automate that). Once you have CPLEX configure, you need to run the follow commands to be able to run LP-Recognizer:

```bash
git clone https://github.com/pucrs-automated-planning/lp-recognizer.git
bash prepare-fd.sh #This should download Fast Downward apply patches and compile
```

## Commit hacks (changes) to Fast Downward here

We store our changes to Fast Downward in the ```fd-patch.diff``` patch file in this repository. Whenever you change Fast Downward for LP-Recognizer, ensure these changes are stored here by running:
```bash
bash make-fd-patch.sh
git commit -m "Storing patches to Fast Downward" fd-patch.diff fd-patch-rev.txt
git push
```  

## Running LP-Recognizer

### Running plan recognition in a single problem

Our Python code was based off of Ramirez and Geffner's original recognizer, so experiments are in the form of a ```tar.bz2``` file containing:

- A PDDL Domain
- An observation sequence
- A number of hypotheses (possible goal formulas)
- A PDDL Problem Template (containing the initial state)

This recognizer is compatible with all the domains in this publicly available dataset of [Goal and Plan Recognition Datasets](https://github.com/pucrs-automated-planning/goal-plan-recognition-dataset). We currently implement four different approaches to goal recognition based on operator counts:

1.  Comparing overlap of observations and operator counts, accessible with the ```v``` heuristic
2.  Minimizing the operator counts subject to constraints derived from the observations (i.e. all observations must be included in the counts), accessible with the ```c``` heuristic
3. Minimizing the difference between the operator counts with the observation constraints and the operator counts with then, accessible with the ```dc``` heuristic
4. Minimizing the operator counts subject to soft constraints on the observations (i.e. trying to include as many observations in the counts while minimizing total count), accessible with the ```s``` heuristic

To run any experiment, just run:
```bash
python test_instance.py -r <heuristics> -e <experiment_file>
``` 

Where ```<experiment_file>``` is one of the experiments in your dataset. 
For example, with the experiments we provide here, we could run sokoban with the hard constraints strategy as follows:

```bash
./test_instance.py -r dc -e experiments/sokoban/10/sokoban_p01_hyp-1_10_1.tar.bz2
```

### Running plan recognition in a set of experiments 

In order to run experiments for an entire domain organized in a folder named ```domain```, you need to run:

```bash
./test_domain.py <path> <domain> <heuristics>
```

For example, to run all Sokoban experiments, using all the heuristics you need to run:

```bash
./test_domain.py experiments sokoban v c dc ds
```

### Running plan recognition 

In order to run all of the experiments in our paper ```replace with citation```, you need to run ```experiments/run-all-experiments.sh```, which will run every single domain for all approaches. 